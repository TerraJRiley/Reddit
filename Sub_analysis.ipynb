{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Imports\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Processing libraries, initiations and functions\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re # Delete this if scraping in same notebook\n",
    "#import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 This is a function that scrapes a subreddit and turns it into a pandas dataframe. \n",
    "def fun_scrape_reddit(the_subreddit, pages = 40, verbose = 1):\n",
    "    all_posts = []\n",
    "    first_url = 'http://www.reddit.com/r/' + the_subreddit + '.json'\n",
    "    url = first_url\n",
    "    list_dicts = []\n",
    "    \n",
    "    # Scraping:\n",
    "    for round in range(pages):\n",
    "        res = requests.get(url, headers = {'User-agent':'Electronic Goddess'})\n",
    "        data = res.json()\n",
    "        list_posts = data['data']['children']\n",
    "        \n",
    "        for i in range(len(list_posts)):\n",
    "            index_dictionary = {\n",
    "                    'Name'      : list_posts[i]['data']['name'],\n",
    "                    'Title'     : list_posts[i]['data']['title'],\n",
    "                    'Selftext'  : list_posts[i]['data']['selftext'],\n",
    "                    'Subreddit' : list_posts[i]['data']['subreddit']\n",
    "                }\n",
    "            list_dicts.append(index_dictionary)\n",
    "            \n",
    "        after = data['data']['after']\n",
    "        if type(after) == type(None):\n",
    "            print('Done!')\n",
    "            break\n",
    "        else:\n",
    "            url = first_url +'?after=' + after\n",
    "            if verbose == 1:\n",
    "                print('Round: '+ str(round + 1))\n",
    "            time.sleep(1)\n",
    "\n",
    "    return pd.DataFrame(list_dicts, columns = ['Name','Title','Selftext','Subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "df_gaslighting = fun_scrape_reddit('gaslighting', verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Selftext</th>\n",
       "      <th>Subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>t3_9iblkv</td>\n",
       "      <td>\"Gaslighting\": One of the Most Dangerous Forms...</td>\n",
       "      <td></td>\n",
       "      <td>gaslighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>t3_gad1g1</td>\n",
       "      <td>I think it may be time to let go....</td>\n",
       "      <td>I've been hearing this term \"gaslighting\" a lo...</td>\n",
       "      <td>gaslighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>t3_gaho7j</td>\n",
       "      <td>Thinking about making a rule for the subreddit.</td>\n",
       "      <td>None of us here are therapists or otherwise qu...</td>\n",
       "      <td>gaslighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>t3_ga6w7c</td>\n",
       "      <td>Did she gaslight me?</td>\n",
       "      <td>It was a month, only a month and it was the mo...</td>\n",
       "      <td>gaslighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>t3_ga6lb9</td>\n",
       "      <td>Doctor Gaslighting his ex, also a doctor. \"Cun...</td>\n",
       "      <td></td>\n",
       "      <td>gaslighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>t3_5qkd8a</td>\n",
       "      <td>What we talk about when we talk about Donald T...</td>\n",
       "      <td></td>\n",
       "      <td>gaslighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>t3_5pl6oi</td>\n",
       "      <td>Is this gaslighting or not?</td>\n",
       "      <td></td>\n",
       "      <td>gaslighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>t3_5pi4fl</td>\n",
       "      <td>Alternative Facts</td>\n",
       "      <td></td>\n",
       "      <td>gaslighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>t3_5hmgtv</td>\n",
       "      <td>Gaslighting to Deflect from Unethical Behavior</td>\n",
       "      <td>When your partner gaslights you to deflect fro...</td>\n",
       "      <td>gaslighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>t3_5hmgtu</td>\n",
       "      <td>Gaslighting to Deflect from Unethical Behavior</td>\n",
       "      <td>When your partner gaslights you to deflect fro...</td>\n",
       "      <td>gaslighting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>383 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name                                              Title  \\\n",
       "0    t3_9iblkv  \"Gaslighting\": One of the Most Dangerous Forms...   \n",
       "1    t3_gad1g1               I think it may be time to let go....   \n",
       "2    t3_gaho7j    Thinking about making a rule for the subreddit.   \n",
       "3    t3_ga6w7c                               Did she gaslight me?   \n",
       "4    t3_ga6lb9  Doctor Gaslighting his ex, also a doctor. \"Cun...   \n",
       "..         ...                                                ...   \n",
       "378  t3_5qkd8a  What we talk about when we talk about Donald T...   \n",
       "379  t3_5pl6oi                        Is this gaslighting or not?   \n",
       "380  t3_5pi4fl                                  Alternative Facts   \n",
       "381  t3_5hmgtv     Gaslighting to Deflect from Unethical Behavior   \n",
       "382  t3_5hmgtu     Gaslighting to Deflect from Unethical Behavior   \n",
       "\n",
       "                                              Selftext    Subreddit  \n",
       "0                                                       gaslighting  \n",
       "1    I've been hearing this term \"gaslighting\" a lo...  gaslighting  \n",
       "2    None of us here are therapists or otherwise qu...  gaslighting  \n",
       "3    It was a month, only a month and it was the mo...  gaslighting  \n",
       "4                                                       gaslighting  \n",
       "..                                                 ...          ...  \n",
       "378                                                     gaslighting  \n",
       "379                                                     gaslighting  \n",
       "380                                                     gaslighting  \n",
       "381  When your partner gaslights you to deflect fro...  gaslighting  \n",
       "382  When your partner gaslights you to deflect fro...  gaslighting  \n",
       "\n",
       "[383 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gaslighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Test Scraper\n",
    "def fun_test_scrape(the_subreddit):\n",
    "    url = ('http://www.reddit.com/r/' + \n",
    "        the_subreddit + '.json')\n",
    "    res = requests.get(url, \n",
    "        headers = {'User-agent':'Electronic Goddess'})\n",
    "    data = res.json()\n",
    "    list_of_posts = data['data']['children'][0]\n",
    "    return list_of_posts\n",
    "\n",
    "#fun_test_scrape('AskReddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "# 2 Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "df_fds = fun_scrape_reddit('FemaleDatingStrategy',verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Selftext</th>\n",
       "      <th>Subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>t3_g6ydky</td>\n",
       "      <td>COVID-19 [Megathread] Week of April 23-April 29</td>\n",
       "      <td>Currently a pandemic called [COVID-19](https:/...</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>t3_gaarw1</td>\n",
       "      <td>What is a movie you find terrible but critics ...</td>\n",
       "      <td></td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>t3_gabx37</td>\n",
       "      <td>Redditors who have actully gotten married, how...</td>\n",
       "      <td></td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>t3_ga9q25</td>\n",
       "      <td>\"winning an argument against a genius is hard,...</td>\n",
       "      <td></td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>t3_gacbsj</td>\n",
       "      <td>Breaking News, an A-list celebrity has been ar...</td>\n",
       "      <td></td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>987</td>\n",
       "      <td>t3_gagkvg</td>\n",
       "      <td>The newest patch notes (May 2020) for Earth ha...</td>\n",
       "      <td></td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>t3_gagktb</td>\n",
       "      <td>What is the most important show in television ...</td>\n",
       "      <td></td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>t3_gagkt1</td>\n",
       "      <td>Why are you using reddit? How did you find red...</td>\n",
       "      <td></td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>t3_gagks6</td>\n",
       "      <td>What is the weirdest roommate that you ever ha...</td>\n",
       "      <td></td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991</td>\n",
       "      <td>t3_gaj3dq</td>\n",
       "      <td>The first thing you think of kills you, What w...</td>\n",
       "      <td></td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>992 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name                                              Title  \\\n",
       "0    t3_g6ydky    COVID-19 [Megathread] Week of April 23-April 29   \n",
       "1    t3_gaarw1  What is a movie you find terrible but critics ...   \n",
       "2    t3_gabx37  Redditors who have actully gotten married, how...   \n",
       "3    t3_ga9q25  \"winning an argument against a genius is hard,...   \n",
       "4    t3_gacbsj  Breaking News, an A-list celebrity has been ar...   \n",
       "..         ...                                                ...   \n",
       "987  t3_gagkvg  The newest patch notes (May 2020) for Earth ha...   \n",
       "988  t3_gagktb  What is the most important show in television ...   \n",
       "989  t3_gagkt1  Why are you using reddit? How did you find red...   \n",
       "990  t3_gagks6  What is the weirdest roommate that you ever ha...   \n",
       "991  t3_gaj3dq  The first thing you think of kills you, What w...   \n",
       "\n",
       "                                              Selftext  Subreddit  \n",
       "0    Currently a pandemic called [COVID-19](https:/...  AskReddit  \n",
       "1                                                       AskReddit  \n",
       "2                                                       AskReddit  \n",
       "3                                                       AskReddit  \n",
       "4                                                       AskReddit  \n",
       "..                                                 ...        ...  \n",
       "987                                                     AskReddit  \n",
       "988                                                     AskReddit  \n",
       "989                                                     AskReddit  \n",
       "990                                                     AskReddit  \n",
       "991                                                     AskReddit  \n",
       "\n",
       "[992 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AskReddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling Nulls\n",
    "les_or_inc.fillna('', inplace=True)\n",
    "\n",
    "# Combining the title and selftext columns\n",
    "les_or_inc['all_text'] = les_or_inc['title'] + ' ' + les_or_inc['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "les_or_inc.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Count Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiations of the tokenizer, lemmatizer and Count Vectorizer (with preprocessor)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([lemmer.lemmatize(word\n",
    "                        ) for word in tokens if len(word) > 1 and not word in stop_words])\n",
    "cvec = CountVectorizer(analyzer     = \"word\",\n",
    "                       tokenizer    = tokenizer.tokenize,\n",
    "                       preprocessor = preprocess,\n",
    "                       stop_words   = 'english',\n",
    "                       min_df       = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pearl/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'u'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# Creating Cvec DataFrame of both forums\n",
    "df_words = pd.DataFrame(cvec.fit_transform(df_fds['Title'] + ' ' + df_fds['Selftext']).todense(), \n",
    "                        columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youtu</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtuber</th>\n",
       "      <th>youtubers</th>\n",
       "      <th>yr</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zvm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>832 rows × 3850 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aa  abandon  abandoned  ability  able  abortion  abroad  absence  \\\n",
       "0     0        0          0        0     0         0       0        0   \n",
       "1     0        0          1        0     0         5       0        0   \n",
       "2     0        0          0        0     0         0       0        0   \n",
       "3     0        0          0        0     0         0       0        0   \n",
       "4     0        0          0        0     0         0       0        0   \n",
       "..   ..      ...        ...      ...   ...       ...     ...      ...   \n",
       "827   0        0          0        0     0         0       0        0   \n",
       "828   0        0          0        0     0         0       0        0   \n",
       "829   0        0          0        0     0         0       0        0   \n",
       "830   0        0          0        0     0         0       0        0   \n",
       "831   0        0          0        0     0         0       0        0   \n",
       "\n",
       "     absolute  absolutely  ...  younger  youtu  youtube  youtuber  youtubers  \\\n",
       "0           0           0  ...        0      0        0         0          0   \n",
       "1           0           0  ...        1      0        0         0          0   \n",
       "2           0           0  ...        0      0        0         0          0   \n",
       "3           0           0  ...        0      0        0         0          0   \n",
       "4           0           0  ...        0      0        0         0          0   \n",
       "..        ...         ...  ...      ...    ...      ...       ...        ...   \n",
       "827         0           0  ...        0      0        0         0          0   \n",
       "828         0           0  ...        0      0        0         0          0   \n",
       "829         0           0  ...        0      0        0         0          0   \n",
       "830         0           0  ...        0      0        0         0          0   \n",
       "831         0           0  ...        0      0        2         0          0   \n",
       "\n",
       "     yr  zero  zone  zoom  zvm  \n",
       "0     0     0     0     0    0  \n",
       "1     0     0     0     0    0  \n",
       "2     0     0     0     0    0  \n",
       "3     0     0     0     0    0  \n",
       "4     0     0     0     0    0  \n",
       "..   ..   ...   ...   ...  ...  \n",
       "827   0     0     0     0    0  \n",
       "828   0     0     0     0    0  \n",
       "829   0     0     0     0    0  \n",
       "830   0     0     0     0    0  \n",
       "831   0     0     0     0    0  \n",
       "\n",
       "[832 rows x 3850 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 E. D. A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "def LDA(input_item, num_topics = 3, num_words = 5, pre_cveced = False):\n",
    "    # Cols are the words. Rows are the topics\n",
    "    topic_lists = []\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, learning_method='online')\n",
    "    \n",
    "    if  pre_cveced == False: # For inserting a column and automatically cvecing things in function.\n",
    "        lda.fit(cvec.fit_transform(input_item))\n",
    "    elif pre_cveced == True: # For inserting a pre-cveced dataframe.\n",
    "        lda.fit(input_item)\n",
    "    else:\n",
    "        print(\"pre_cveced only takes True or False\")\n",
    "        return\n",
    "    for ix, topic in enumerate(lda.components_):\n",
    "        topic_lists += [[cvec.get_feature_names()[i] for i \n",
    "                         in lda.components_[ix].argsort()[:-num_words - 1:-1]]]\n",
    "\n",
    "    return pd.DataFrame(topic_lists, \n",
    "                        columns=[ 'Word_'  + str(i) for i in range(1, num_words +1 )],\n",
    "                        index = [ 'Topic_' + str(i) for i in range(1, num_topics + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_1</th>\n",
       "      <th>Word_2</th>\n",
       "      <th>Word_3</th>\n",
       "      <th>Word_4</th>\n",
       "      <th>Word_5</th>\n",
       "      <th>Word_6</th>\n",
       "      <th>Word_7</th>\n",
       "      <th>Word_8</th>\n",
       "      <th>Word_9</th>\n",
       "      <th>Word_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Topic_1</td>\n",
       "      <td>scrote</td>\n",
       "      <td>endorse</td>\n",
       "      <td>gay</td>\n",
       "      <td>content</td>\n",
       "      <td>yes</td>\n",
       "      <td>relevant</td>\n",
       "      <td>sub</td>\n",
       "      <td>receipt</td>\n",
       "      <td>artist</td>\n",
       "      <td>fds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic_2</td>\n",
       "      <td>time</td>\n",
       "      <td>like</td>\n",
       "      <td>relationship</td>\n",
       "      <td>men</td>\n",
       "      <td>want</td>\n",
       "      <td>know</td>\n",
       "      <td>woman</td>\n",
       "      <td>man</td>\n",
       "      <td>year</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic_3</td>\n",
       "      <td>like</td>\n",
       "      <td>woman</td>\n",
       "      <td>guy</td>\n",
       "      <td>men</td>\n",
       "      <td>want</td>\n",
       "      <td>feel</td>\n",
       "      <td>friend</td>\n",
       "      <td>sex</td>\n",
       "      <td>think</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic_4</td>\n",
       "      <td>amp</td>\n",
       "      <td>http</td>\n",
       "      <td>png</td>\n",
       "      <td>lvm</td>\n",
       "      <td>age</td>\n",
       "      <td>webp</td>\n",
       "      <td>preview</td>\n",
       "      <td>redd</td>\n",
       "      <td>format</td>\n",
       "      <td>width</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic_5</td>\n",
       "      <td>woman</td>\n",
       "      <td>men</td>\n",
       "      <td>amp</td>\n",
       "      <td>sociopath</td>\n",
       "      <td>beware</td>\n",
       "      <td>abuse</td>\n",
       "      <td>power</td>\n",
       "      <td>male</td>\n",
       "      <td>consciousness</td>\n",
       "      <td>sexuality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word_1   Word_2        Word_3     Word_4  Word_5    Word_6   Word_7  \\\n",
       "Topic_1  scrote  endorse           gay    content     yes  relevant      sub   \n",
       "Topic_2    time     like  relationship        men    want      know    woman   \n",
       "Topic_3    like    woman           guy        men    want      feel   friend   \n",
       "Topic_4     amp     http           png        lvm     age      webp  preview   \n",
       "Topic_5   woman      men           amp  sociopath  beware     abuse    power   \n",
       "\n",
       "          Word_8         Word_9    Word_10  \n",
       "Topic_1  receipt         artist        fds  \n",
       "Topic_2      man           year       life  \n",
       "Topic_3      sex          think       know  \n",
       "Topic_4     redd         format      width  \n",
       "Topic_5     male  consciousness  sexuality  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA(df_fds['Title'] + df_fds['Selftext'], num_topics= 5, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Laboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentament\n",
    "def Sentamentize(text):\n",
    "    return TextBlob(str(text)).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Words\n",
    "def most_common_words(cveced_df):\n",
    "    return cveced_df.sum().sort_values()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations between two DFs\n",
    "# Listing the correlations to the two data frames.\n",
    "# 1 = represents coming from 1st subreddit.\n",
    "# 0 = represents coming from 2nd subreddit.\n",
    "def correlation_of_words(cvec_dfs, target_title = \"is_first_df\")\n",
    "    return df_words.corr().sort_values([target_title])[target_title]\n",
    "# df_corrs.tail(20)[18::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Experiment: Allowing LDA function to be pre-cveced and in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original LDA\n",
    "def LDA(input_item, num_topics = 3, num_words = 5, pre_cveced = False):\n",
    "    # Cols are the words. Rows are the topics\n",
    "    topic_lists = []\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, learning_method='online')\n",
    "    \n",
    "    if  pre_cveced == False: # For inserting a column and automatically cvecing things in function.\n",
    "        lda.fit(cvec.fit_transform(input_item))\n",
    "    elif pre_cveced == True: # For inserting a pre-cveced dataframe.\n",
    "        lda.fit(input_item)\n",
    "    else:\n",
    "        print(\"pre_cveced only takes True or False\")\n",
    "        return\n",
    "    for ix, topic in enumerate(lda.components_):\n",
    "        topic_lists += [[cvec.get_feature_names()[i] for i \n",
    "                         in lda.components_[ix].argsort()[:-num_words - 1:-1]]]\n",
    "\n",
    "    return pd.DataFrame(topic_lists, \n",
    "                        columns=[ 'Word_' + str(i) for i in range(1, num_words+1)],\n",
    "                        index = [ 'Topic_' + str(i) for i in range(1, num_topics + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pearl/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'u'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>study</td>\n",
       "      <td>government</td>\n",
       "      <td>free</td>\n",
       "      <td>sars</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>covid</td>\n",
       "      <td>test</td>\n",
       "      <td>testing</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>covid</td>\n",
       "      <td>death</td>\n",
       "      <td>case</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>vaccine</td>\n",
       "      <td>reopen</td>\n",
       "      <td>stay</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>covid</td>\n",
       "      <td>pandemic</td>\n",
       "      <td>virus</td>\n",
       "      <td>mask</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0           1         2        3         4\n",
       "0        study  government      free     sars       say\n",
       "1  coronavirus       covid      test  testing         u\n",
       "2  coronavirus       covid     death     case       new\n",
       "3  coronavirus     vaccine    reopen     stay  business\n",
       "4  coronavirus       covid  pandemic    virus      mask"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting work on allowing function to be pre-cveced and in a DataFrame\n",
    "num_topics = 3\n",
    "    # Cols are the words. Rows are the topics\n",
    "topic_lists = []\n",
    "lda = LatentDirichletAllocation(n_components=5, learning_method='online')\n",
    "lda.fit(cvec.fit_transform((df_Coronavirus['Title'] + df_Coronavirus['Selftext'])))\n",
    "\n",
    "for ix, topic in enumerate(lda.components_):\n",
    "    topic_lists += [[cvec.get_feature_names()[i] for i in lda.components_[ix].argsort()[:-5 - 1:-1]]]\n",
    "\n",
    "pd.DataFrame(topic_lists)#, columns=[ 'word_' + str(i) for i # Comment this out for ease.\n",
    "                                   # in range(1, 6)], index=range(1, num_topics + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Experiment: Jargon Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Cut out words used in AskReddit Sub? (If \"girl\" has been used once or more in Ask, then cut it out of the Question DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns correlated words vs Ask\n",
    "test_value = df_fds\n",
    "#print(test_value.head())\n",
    "def fun_(val_in, df_ctrl = df_AskReddit):\n",
    "    val_in['is_target'] = 1\n",
    "    df_ctrl['is_target']= 0\n",
    "    df_jargon = pd.concat([ val_in.drop('Subreddit', axis=1),\n",
    "                            df_ctrl.drop('Subreddit', axis=1)],\n",
    "                            sort=True).reset_index()\n",
    "    \n",
    "    df_words = pd.DataFrame(cvec.fit_transform(\n",
    "        df_jargon['Title'] + ' ' + df_jargon['Selftext']).todense(), \n",
    "        columns=cvec.get_feature_names())\n",
    "    df_words['is_target'] = df_jargon['is_target']\n",
    "    df_corrs = df_words.corr().sort_values(['is_target'])['is_target']\n",
    "    return df_corrs.drop('is_target')\n",
    "test = fun_(test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "want       0.300836\n",
       "know       0.298594\n",
       "like       0.294275\n",
       "dating     0.263467\n",
       "men        0.261495\n",
       "guy        0.259197\n",
       "time       0.255947\n",
       "really     0.251466\n",
       "think      0.245828\n",
       "thought    0.228016\n",
       "woman      0.226616\n",
       "year       0.222725\n",
       "man        0.219374\n",
       "way        0.217354\n",
       "feel       0.216444\n",
       "going      0.214858\n",
       "need       0.213034\n",
       "make       0.212173\n",
       "say        0.211214\n",
       "got        0.211015\n",
       "Name: is_target, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sort_values(ascending= False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get jargon\n",
    "test_value = df_fds\n",
    "#print(test_value.head())\n",
    "def fun_(val_in, df_ctrl = df_AskReddit):\n",
    "    val_in['is_target'] = 1\n",
    "    df_ctrl['is_target']= 0\n",
    "    df_jargon = pd.concat([ val_in.drop('Subreddit', axis=1),\n",
    "                            df_ctrl.drop('Subreddit', axis=1)],\n",
    "                            sort=True).reset_index()\n",
    "    \n",
    "    df_words = pd.DataFrame(cvec.fit_transform(\n",
    "        df_jargon['Title'] + ' ' + df_jargon['Selftext']).todense(), \n",
    "        columns=cvec.get_feature_names())\n",
    "    df_words['is_target'] = df_jargon['is_target']\n",
    "    return df_words\n",
    "test_df = fun_(test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "able           6\n",
       "absolutely     5\n",
       "actor          7\n",
       "actually      24\n",
       "adult          6\n",
       "              ..\n",
       "word           9\n",
       "work          12\n",
       "world         28\n",
       "worst         27\n",
       "year          24\n",
       "Length: 224, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = test_df['is_target'] == 0\n",
    "mask_words = test_df[mask].sum() > 4\n",
    "test_df[mask].sum()[mask_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    #words_all = stopwords.words(\"english\")\n",
    "    list_clean = [lemmer.lemmatize(word) for word in tokens]\n",
    "    #list_cut   = [word for word in list_clean if not word in words.words()]\n",
    "    return \" \".join(list_clean)\n",
    "# )\n",
    "cvec = CountVectorizer(analyzer     = \"word\",\n",
    "                       tokenizer    = tokenizer.tokenize,\n",
    "                       preprocessor = preprocess,\n",
    "                       stop_words   = None,\n",
    "                       min_df       = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_series = df_fds['Title'] + ' ' + df_fds['Selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cvec = cvec.fit_transform(test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(test_cvec.todense(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415        0\n",
       "554        0\n",
       "291        0\n",
       "290        0\n",
       "289        0\n",
       "       ...  \n",
       "773     9786\n",
       "498    12361\n",
       "616    14116\n",
       "76     20295\n",
       "639    26570\n",
       "Name: Selftext, Length: 832, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fds['Selftext'].map(lambda x: len(x)).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = df_fds.loc[639, 'Selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fds quora asked answered meshing ve needed ll ve heard examp le performs called workaholic changed condo yourselfer energized having tv tv establishing called controlling happened longest divorced exhibited proud talked counseling didn initiating abused assaulted talked viewing honoring suffered transmitted treated addictive suffered etc complimented planned etc changing treated didn didn having alienated identified sabotaged imprinted interferes differing disenfranchised etc having etc nots served served nonviolent committed arrested happened failed tv newsmagazines internet discussing featuring tv ll attending caused socializing planning smarter didn having maintaining caring others carpool internet modem others stronger defers equipped ethnicity dated ethnicity ethnicity aren biased ethnicity ethnicity ethnicity ethnicity showered happens happens greatest greatest creating caring replenishes'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [word for word in df_words.columns if not word in words.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list_two = [word for word in test_list if word[-3:] != 'ing' and word[-2:] != 'ed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abuse',\n",
       " 'achieve',\n",
       " 'acte',\n",
       " 'addresse',\n",
       " 'aligne',\n",
       " 'allowe',\n",
       " 'annoye',\n",
       " 'answere',\n",
       " 'apologize',\n",
       " 'appeare',\n",
       " 'appreciate',\n",
       " 'approache',\n",
       " 'approve',\n",
       " 'argue',\n",
       " 'arreste',\n",
       " 'aske',\n",
       " 'assaulte',\n",
       " 'asse',\n",
       " 'attacke',\n",
       " 'attempte',\n",
       " 'attracte',\n",
       " 'avoide',\n",
       " 'believe',\n",
       " 'betraye',\n",
       " 'bore',\n",
       " 'bothere',\n",
       " 'bullie',\n",
       " 'calle',\n",
       " 'cancele',\n",
       " 'cancelle',\n",
       " 'care',\n",
       " 'cause',\n",
       " 'challenge',\n",
       " 'change',\n",
       " 'chase',\n",
       " 'chatte',\n",
       " 'cheate',\n",
       " 'choke',\n",
       " 'claime',\n",
       " 'cleane',\n",
       " 'commente',\n",
       " 'committe',\n",
       " 'communicate',\n",
       " 'compare',\n",
       " 'complimente',\n",
       " 'conflicte',\n",
       " 'confronte',\n",
       " 'consume',\n",
       " 'contacte',\n",
       " 'contribute',\n",
       " 'cooke',\n",
       " 'create',\n",
       " 'criticize',\n",
       " 'damage',\n",
       " 'dare',\n",
       " 'date',\n",
       " 'delete',\n",
       " 'delivere',\n",
       " 'delude',\n",
       " 'demonstrate',\n",
       " 'describe',\n",
       " 'destroye',\n",
       " 'devastate',\n",
       " 'develope',\n",
       " 'die',\n",
       " 'discusse',\n",
       " 'disenfranchise',\n",
       " 'disrespecte',\n",
       " 'ditche',\n",
       " 'divorce',\n",
       " 'downvote',\n",
       " 'droppe',\n",
       " 'dumpe',\n",
       " 'e',\n",
       " 'emaile',\n",
       " 'empowere',\n",
       " 'encourage',\n",
       " 'energize',\n",
       " 'enjoye',\n",
       " 'entere',\n",
       " 'entitle',\n",
       " 'equippe',\n",
       " 'existe',\n",
       " 'expecte',\n",
       " 'explaine',\n",
       " 'exploite',\n",
       " 'faile',\n",
       " 'flattere',\n",
       " 'focuse',\n",
       " 'followe',\n",
       " 'foole',\n",
       " 'frustrate',\n",
       " 'fucke',\n",
       " 'ghoste',\n",
       " 'grabbe',\n",
       " 'grante',\n",
       " 'greete',\n",
       " 'groome',\n",
       " 'happene',\n",
       " 'hate',\n",
       " 'helpe',\n",
       " 'hoste',\n",
       " 'humiliate',\n",
       " 'ignore',\n",
       " 'imagine',\n",
       " 'implie',\n",
       " 'impresse',\n",
       " 'improve',\n",
       " 'increase',\n",
       " 'indoctrinate',\n",
       " 'influence',\n",
       " 'initiate',\n",
       " 'insiste',\n",
       " 'insulte',\n",
       " 'intensifie',\n",
       " 'interacte',\n",
       " 'introduce',\n",
       " 'investe',\n",
       " 'invite',\n",
       " 'irritate',\n",
       " 'joine',\n",
       " 'justifie',\n",
       " 'kille',\n",
       " 'kisse',\n",
       " 'labele',\n",
       " 'lacke',\n",
       " 'laste',\n",
       " 'laughe',\n",
       " 'launche',\n",
       " 'liberate',\n",
       " 'like',\n",
       " 'listene',\n",
       " 'looke',\n",
       " 'love',\n",
       " 'maintaine',\n",
       " 'manipulate',\n",
       " 'matche',\n",
       " 'mattere',\n",
       " 'me',\n",
       " 'mentione',\n",
       " 'message',\n",
       " 'misse',\n",
       " 'motivate',\n",
       " 'move',\n",
       " 'name',\n",
       " 'neede',\n",
       " 'negge',\n",
       " 'neglecte',\n",
       " 'normalise',\n",
       " 'notice',\n",
       " 'nuance',\n",
       " 'objectifie',\n",
       " 'obligate',\n",
       " 'observe',\n",
       " 'obsesse',\n",
       " 'occurre',\n",
       " 'offere',\n",
       " 'opene',\n",
       " 'owe',\n",
       " 'passe',\n",
       " 'penetrate',\n",
       " 'perceive',\n",
       " 'performe',\n",
       " 'pisse',\n",
       " 'planne',\n",
       " 'playe',\n",
       " 'portraye',\n",
       " 'praise',\n",
       " 'presente',\n",
       " 'presse',\n",
       " 'pressure',\n",
       " 'proceede',\n",
       " 'publishe',\n",
       " 'pulle',\n",
       " 'punishe',\n",
       " 'purchase',\n",
       " 'pursue',\n",
       " 'pushe',\n",
       " 'quarantine',\n",
       " 'rape',\n",
       " 'reache',\n",
       " 'realise',\n",
       " 'realize',\n",
       " 'recommende',\n",
       " 'reference',\n",
       " 'refuse',\n",
       " 'regrette',\n",
       " 'rejecte',\n",
       " 'remaine',\n",
       " 'remembere',\n",
       " 'reminde',\n",
       " 'repelle',\n",
       " 'replie',\n",
       " 'reporte',\n",
       " 'reposte',\n",
       " 'repulse',\n",
       " 'require',\n",
       " 'reschedule',\n",
       " 'respecte',\n",
       " 'responde',\n",
       " 'scare',\n",
       " 'searche',\n",
       " 'seeme',\n",
       " 'separate',\n",
       " 'sexualize',\n",
       " 'share',\n",
       " 'shocke',\n",
       " 'showe',\n",
       " 'slappe',\n",
       " 'starte',\n",
       " 'stresse',\n",
       " 'struggle',\n",
       " 'stumble',\n",
       " 'sucke',\n",
       " 'suffere',\n",
       " 'suggeste',\n",
       " 'supporte',\n",
       " 'surprise',\n",
       " 'talke',\n",
       " 'tempte',\n",
       " 'texte',\n",
       " 'thanke',\n",
       " 'theme',\n",
       " 'threatene',\n",
       " 'thrille',\n",
       " 'transmitte',\n",
       " 'treate',\n",
       " 'truste',\n",
       " 'update',\n",
       " 'viewe',\n",
       " 'violate',\n",
       " 'walke',\n",
       " 'wante',\n",
       " 'warne',\n",
       " 'welcome',\n",
       " 'wondere']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[word[:-3] for word in test_list if word[-3:] != 'ing']\n",
    "[word[:-2] for word in test_list if word[-2:] == 'ed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accept'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[3][:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Jargon Ideas:\n",
    "\n",
    " - Attempt Stemming?\n",
    " - Remove any/all words in list of subreddits\n",
    " - Re-insert list of words and use as a filter for the column count of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recycling Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value = \n",
    "print(test_value)\n",
    "def fun_(val_in):\n",
    "    return val_out\n",
    "fun_(test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "def fun_display_pic(urll):\n",
    "    return display(Image( url= urll))\n",
    "\n",
    "#display(Image(url=bro_fun.find_element_by_css_selector(ATS_dict['logo']\n",
    "#                ).get_attribute('src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Takes in the post url and spits out the image url\n",
    "def fun_url_to_img(url):\n",
    "    html = BeautifulSoup(requests.get(url, headers = {'User-agent':'Electronic Goddess'}).content)\n",
    "    element = html.select('div[data-test-id=\"post-content\"] > div > a > img')[0]\n",
    "    return element.attrs['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Sanity Check:\n",
    "quick_check = requests.get(first_url, headers = {'User-agent':'Electronic Goddess'})\n",
    "if int(str(quick_check)[11:14]) == 200:\n",
    "    print(\"Get request successful.\")\n",
    "    time.sleep(3)\n",
    "    print(\"Initiating Scrape...\")\n",
    "else:\n",
    "    print(\"Get request not 200, instead recieved:\" + str(quick_check))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Collecting TextBlob\n",
      "  Using cached https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl\n",
      "Processing /Users/pearl/Library/Caches/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306/nltk-3.5-cp27-none-any.whl\n",
      "Collecting click\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl\n",
      "Collecting joblib\n",
      "  Using cached https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl\n",
      "Processing /Users/pearl/Library/Caches/pip/wheels/e6/9b/ae/2972da29cc7759b71dee015813b7c6931917d6a51e64ed5e79/regex-2020.4.4-cp27-cp27m-macosx_10_15_x86_64.whl\n",
      "Collecting tqdm\n",
      "  Using cached https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl\n",
      "Installing collected packages: click, joblib, regex, tqdm, nltk, TextBlob\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/Users/pearl/Library/Python/2.7/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/Users/pearl/Library/Python/2.7/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed TextBlob-0.15.3 click-7.1.2 joblib-0.14.1 nltk-3.5 regex-2020.4.4 tqdm-4.45.0\n"
     ]
    }
   ],
   "source": [
    "!pip install TextBlob --user # --user was needed here for the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X Experiment: Getting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X Experiment: Getting Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Round: 1\n",
      "Round: 2\n",
      "Round: 3\n",
      "Round: 4\n",
      "Round: 5\n",
      "Round: 6\n",
      "Round: 7\n",
      "Round: 8\n",
      "Round: 9\n",
      "Round: 10\n",
      "Round: 11\n",
      "Round: 12\n",
      "Round: 13\n",
      "Round: 14\n",
      "Round: 15\n",
      "Round: 16\n",
      "Round: 17\n",
      "Round: 18\n",
      "Round: 19\n",
      "Round: 20\n",
      "Round: 21\n",
      "Round: 22\n",
      "Round: 23\n",
      "Round: 24\n",
      "Round: 25\n",
      "Round: 26\n",
      "Round: 27\n",
      "Round: 28\n",
      "Round: 29\n",
      "Round: 30\n",
      "Round: 31\n",
      "Round: 32\n",
      "Round: 33\n",
      "Round: 34\n",
      "Round: 35\n",
      "Round: 36\n",
      "Round: 37\n",
      "Limit likely hit.  Returning available posts.\n"
     ]
    }
   ],
   "source": [
    "df_libertarian = scrape_reddit(\"Libertarian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Selftext</th>\n",
       "      <th>Subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>t3_g5xrrf</td>\n",
       "      <td>Is this what our forefathers died for...?</td>\n",
       "      <td>The state should never misinterpret itself as ...</td>\n",
       "      <td>Libertarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>t3_g5ove2</td>\n",
       "      <td>New to libertarianism, how would that solve in...</td>\n",
       "      <td>From what I understand (which is not much sinc...</td>\n",
       "      <td>Libertarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906</td>\n",
       "      <td>t3_g5wgre</td>\n",
       "      <td>George Papadopoulos collusion denials to ‘Spyg...</td>\n",
       "      <td>[https://www.washingtonexaminer.com/news/geor...</td>\n",
       "      <td>Libertarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>907</td>\n",
       "      <td>t3_g5m0x3</td>\n",
       "      <td>Laws at the whim of bureaucrats</td>\n",
       "      <td>\"When men are caught in the trap of non-object...</td>\n",
       "      <td>Libertarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>t3_g5g0kk</td>\n",
       "      <td>Do Libertarians believe in anti-trust laws?</td>\n",
       "      <td>Hey, just trying to educate myself on Libertar...</td>\n",
       "      <td>Libertarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>911</td>\n",
       "      <td>t3_g59q28</td>\n",
       "      <td>I want a tax cut for all the government servic...</td>\n",
       "      <td>Is anyone else a little miffed that we're stil...</td>\n",
       "      <td>Libertarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>916</td>\n",
       "      <td>t3_g5j1lt</td>\n",
       "      <td>Reliable news sources</td>\n",
       "      <td>Can anyone suggest credible political news sou...</td>\n",
       "      <td>Libertarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>919</td>\n",
       "      <td>t3_g62k05</td>\n",
       "      <td>So are we expected to shut down our lives just...</td>\n",
       "      <td>Are we going to bend over every time the gover...</td>\n",
       "      <td>Libertarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>t3_g61hc7</td>\n",
       "      <td>Just a reminder, we have had a vaccine for COV...</td>\n",
       "      <td>Moderna had a vaccine on January 13th, and [pr...</td>\n",
       "      <td>Libertarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>t3_g5si37</td>\n",
       "      <td>Libertarian and Fully Free Market Minecraft Se...</td>\n",
       "      <td>**TCB**  \\n***Economy, Claims, and Freedom!**...</td>\n",
       "      <td>Libertarian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name                                              Title  \\\n",
       "898  t3_g5xrrf          Is this what our forefathers died for...?   \n",
       "900  t3_g5ove2  New to libertarianism, how would that solve in...   \n",
       "906  t3_g5wgre  George Papadopoulos collusion denials to ‘Spyg...   \n",
       "907  t3_g5m0x3                    Laws at the whim of bureaucrats   \n",
       "910  t3_g5g0kk        Do Libertarians believe in anti-trust laws?   \n",
       "911  t3_g59q28  I want a tax cut for all the government servic...   \n",
       "916  t3_g5j1lt                              Reliable news sources   \n",
       "919  t3_g62k05  So are we expected to shut down our lives just...   \n",
       "920  t3_g61hc7  Just a reminder, we have had a vaccine for COV...   \n",
       "936  t3_g5si37  Libertarian and Fully Free Market Minecraft Se...   \n",
       "\n",
       "                                              Selftext    Subreddit  \n",
       "898  The state should never misinterpret itself as ...  Libertarian  \n",
       "900  From what I understand (which is not much sinc...  Libertarian  \n",
       "906   [https://www.washingtonexaminer.com/news/geor...  Libertarian  \n",
       "907  \"When men are caught in the trap of non-object...  Libertarian  \n",
       "910  Hey, just trying to educate myself on Libertar...  Libertarian  \n",
       "911  Is anyone else a little miffed that we're stil...  Libertarian  \n",
       "916  Can anyone suggest credible political news sou...  Libertarian  \n",
       "919  Are we going to bend over every time the gover...  Libertarian  \n",
       "920  Moderna had a vaccine on January 13th, and [pr...  Libertarian  \n",
       "936   **TCB**  \\n***Economy, Claims, and Freedom!**...  Libertarian  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = df_libertarian['Selftext'] != ''\n",
    "df_libertarian[mask].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = ('http://www.reddit.com/r/Libertarian/comments/g59q28.json')\n",
    "\n",
    "#url = ('http://www.reddit.com/r/Libertarian/comments/g62k05.json')\n",
    "res = requests.get(url, headers = {'User-agent':'Electronic Goddess'})\n",
    "test = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test[1]['data']['children'][4]#['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 This is a function that scrapes a subreddit and turns it into a pandas dataframe. \n",
    "def fun_scrape_reddit(the_subreddit, pages = 40):\n",
    "    all_posts = []\n",
    "    first_url = 'http://www.reddit.com/r/' + the_subreddit + '.json'\n",
    "    url = first_url\n",
    "    list_of_df = []\n",
    "    after = 'First'\n",
    "    \n",
    "    # Scraping:\n",
    "    for round in range(pages):\n",
    "        res = requests.get(url, headers = {'User-agent':'Electronic Goddess'})\n",
    "        data = res.json()\n",
    "        list_of_posts = data['data']['children']\n",
    "        all_posts = all_posts + list_of_posts\n",
    "        after = data['data']['after']\n",
    "        if type(after) == type(None):\n",
    "            print('Limit likely hit.  Returning available posts.')\n",
    "            break\n",
    "        else:\n",
    "            url = first_url +'?after=' + after\n",
    "            print(url + '  Round: '+ str(round + 1) + after)\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Formats the parts we care about into a list of dictionaries that'll become the dataframe\n",
    "    for i in range(len(all_posts)):\n",
    "        index_dictionary = {\n",
    "                'Name'      : all_posts[i]['data']['name'],\n",
    "                'Title'     : all_posts[i]['data']['title'],\n",
    "                'Selftext'  : all_posts[i]['data']['selftext'],\n",
    "                'Subreddit' : all_posts[i]['data']['subreddit']\n",
    "            }\n",
    "        list_of_df.append(index_dictionary)\n",
    "    return pd.DataFrame(list_of_df, columns = ['Name','Title','Selftext','Subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Round: 1\n",
      "Round: 2\n",
      "Round: 3\n",
      "Round: 4\n",
      "Round: 5\n",
      "Round: 6\n",
      "Round: 7\n",
      "Round: 8\n",
      "Round: 9\n",
      "Round: 10\n",
      "Round: 11\n",
      "Round: 12\n",
      "Round: 13\n",
      "Round: 14\n",
      "Round: 15\n",
      "Round: 16\n",
      "Round: 17\n",
      "Round: 18\n",
      "Round: 19\n",
      "Round: 20\n",
      "Round: 21\n",
      "Round: 22\n",
      "Round: 23\n",
      "Round: 24\n",
      "Round: 25\n",
      "Round: 26\n",
      "Round: 27\n",
      "Round: 28\n",
      "Round: 29\n",
      "Round: 30\n",
      "Round: 31\n",
      "Round: 32\n",
      "Round: 33\n",
      "Round: 34\n",
      "Round: 35\n",
      "Round: 36\n",
      "Round: 37\n",
      "Round: 38\n",
      "Round: 39\n",
      "Limit likely hit.  Returning available posts.\n"
     ]
    }
   ],
   "source": [
    "df_AskReddit = scrape_reddit('AskReddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Round: 1\n",
      "Round: 2\n",
      "Round: 3\n",
      "Round: 4\n",
      "Round: 5\n",
      "Round: 6\n",
      "Round: 7\n",
      "Round: 8\n",
      "Round: 9\n",
      "Round: 10\n",
      "Round: 11\n",
      "Round: 12\n",
      "Round: 13\n",
      "Round: 14\n",
      "Round: 15\n",
      "Round: 16\n",
      "Round: 17\n",
      "Round: 18\n",
      "Round: 19\n",
      "Round: 20\n",
      "Round: 21\n",
      "Round: 22\n",
      "Round: 23\n",
      "Round: 24\n",
      "Limit likely hit.  Returning available posts.\n"
     ]
    }
   ],
   "source": [
    "df_tifu = scrape_reddit('tifu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Round: 1\n",
      "Round: 2\n",
      "Round: 3\n",
      "Round: 4\n",
      "Round: 5\n",
      "Round: 6\n",
      "Round: 7\n",
      "Round: 8\n",
      "Round: 9\n",
      "Round: 10\n",
      "Round: 11\n",
      "Round: 12\n",
      "Round: 13\n",
      "Round: 14\n",
      "Round: 15\n",
      "Round: 16\n",
      "Round: 17\n",
      "Round: 18\n",
      "Round: 19\n",
      "Round: 20\n",
      "Round: 21\n",
      "Round: 22\n",
      "Round: 23\n",
      "Round: 24\n",
      "Round: 25\n",
      "Round: 26\n",
      "Round: 27\n",
      "Round: 28\n",
      "Round: 29\n",
      "Round: 30\n",
      "Round: 31\n",
      "Round: 32\n",
      "Round: 33\n",
      "Round: 34\n",
      "Round: 35\n",
      "Round: 36\n",
      "Round: 37\n",
      "Round: 38\n",
      "Round: 39\n",
      "Limit likely hit.  Returning available posts.\n"
     ]
    }
   ],
   "source": [
    "df_IncelsWithoutHate = scrape_reddit('IncelsWithoutHate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Round: 1\n",
      "Round: 2\n",
      "Round: 3\n",
      "Round: 4\n",
      "Round: 5\n",
      "Round: 6\n",
      "Round: 7\n",
      "Round: 8\n",
      "Round: 9\n",
      "Round: 10\n",
      "Round: 11\n",
      "Round: 12\n",
      "Round: 13\n",
      "Round: 14\n",
      "Round: 15\n",
      "Round: 16\n",
      "Round: 17\n",
      "Round: 18\n",
      "Round: 19\n",
      "Round: 20\n",
      "Round: 21\n",
      "Round: 22\n",
      "Round: 23\n",
      "Round: 24\n",
      "Round: 25\n",
      "Round: 26\n",
      "Round: 27\n",
      "Round: 28\n",
      "Round: 29\n",
      "Round: 30\n",
      "Round: 31\n",
      "Round: 32\n",
      "Limit likely hit.  Returning available posts.\n"
     ]
    }
   ],
   "source": [
    "df_Coronavirus = scrape_reddit('Coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Round: 1\n",
      "Round: 2\n",
      "Round: 3\n",
      "Round: 4\n",
      "Round: 5\n",
      "Round: 6\n",
      "Round: 7\n",
      "Round: 8\n",
      "Round: 9\n",
      "Round: 10\n",
      "Round: 11\n",
      "Round: 12\n",
      "Round: 13\n",
      "Round: 14\n",
      "Round: 15\n",
      "Round: 16\n",
      "Limit likely hit.  Returning available posts.\n"
     ]
    }
   ],
   "source": [
    "df_transgender = scrape_reddit('transgender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Round: 1\n",
      "Round: 2\n",
      "Round: 3\n",
      "Round: 4\n",
      "Round: 5\n",
      "Round: 6\n",
      "Round: 7\n",
      "Round: 8\n",
      "Round: 9\n",
      "Round: 10\n",
      "Round: 11\n",
      "Round: 12\n",
      "Round: 13\n",
      "Round: 14\n",
      "Round: 15\n",
      "Round: 16\n",
      "Round: 17\n",
      "Round: 18\n",
      "Round: 19\n",
      "Round: 20\n",
      "Round: 21\n",
      "Round: 22\n",
      "Round: 23\n",
      "Round: 24\n",
      "Round: 25\n",
      "Round: 26\n",
      "Round: 27\n",
      "Round: 28\n",
      "Round: 29\n",
      "Round: 30\n",
      "Round: 31\n",
      "Round: 32\n",
      "Round: 33\n",
      "Round: 34\n",
      "Round: 35\n",
      "Round: 36\n",
      "Round: 37\n",
      "Round: 38\n",
      "Round: 39\n",
      "Limit likely hit.  Returning available posts.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Selftext</th>\n",
       "      <th>Subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>t3_bokzzu</td>\n",
       "      <td>A welcome to new users and some FAQ's answered.</td>\n",
       "      <td>Hi there! Welcome to r/Transvoice. Our subscri...</td>\n",
       "      <td>transvoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>t3_d3clhe</td>\n",
       "      <td>L's Voice Training Guide (Level 1) for MTF tra...</td>\n",
       "      <td>L's Voice Training Guide (Level 1) for MTF tra...</td>\n",
       "      <td>transvoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>t3_gb0v9i</td>\n",
       "      <td>i’ve been on the fence about starting t for th...</td>\n",
       "      <td></td>\n",
       "      <td>transvoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>t3_gapin2</td>\n",
       "      <td>I probably sound weird but meh. Since I don't ...</td>\n",
       "      <td></td>\n",
       "      <td>transvoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>t3_gb5rth</td>\n",
       "      <td>I can't use my voice and don't know why</td>\n",
       "      <td>I personally feel this is more on the strange ...</td>\n",
       "      <td>transvoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>t3_er67rm</td>\n",
       "      <td>Breathing issues</td>\n",
       "      <td>I'm mtf very early stages of voice training, a...</td>\n",
       "      <td>transvoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>993</td>\n",
       "      <td>t3_er2zle</td>\n",
       "      <td>Looks like I have work to do.</td>\n",
       "      <td></td>\n",
       "      <td>transvoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>t3_er26l9</td>\n",
       "      <td>Hey guys. Do I pass here and can I get some fe...</td>\n",
       "      <td></td>\n",
       "      <td>transvoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>t3_er81gc</td>\n",
       "      <td>End of first dayof practice and wanted to shar...</td>\n",
       "      <td></td>\n",
       "      <td>transvoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>t3_er6zge</td>\n",
       "      <td>Looking for tips, tricks and advice</td>\n",
       "      <td></td>\n",
       "      <td>transvoice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name                                              Title  \\\n",
       "0    t3_bokzzu    A welcome to new users and some FAQ's answered.   \n",
       "1    t3_d3clhe  L's Voice Training Guide (Level 1) for MTF tra...   \n",
       "2    t3_gb0v9i  i’ve been on the fence about starting t for th...   \n",
       "3    t3_gapin2  I probably sound weird but meh. Since I don't ...   \n",
       "4    t3_gb5rth            I can't use my voice and don't know why   \n",
       "..         ...                                                ...   \n",
       "992  t3_er67rm                                   Breathing issues   \n",
       "993  t3_er2zle                      Looks like I have work to do.   \n",
       "994  t3_er26l9  Hey guys. Do I pass here and can I get some fe...   \n",
       "995  t3_er81gc  End of first dayof practice and wanted to shar...   \n",
       "996  t3_er6zge                Looking for tips, tricks and advice   \n",
       "\n",
       "                                              Selftext   Subreddit  \n",
       "0    Hi there! Welcome to r/Transvoice. Our subscri...  transvoice  \n",
       "1    L's Voice Training Guide (Level 1) for MTF tra...  transvoice  \n",
       "2                                                       transvoice  \n",
       "3                                                       transvoice  \n",
       "4    I personally feel this is more on the strange ...  transvoice  \n",
       "..                                                 ...         ...  \n",
       "992  I'm mtf very early stages of voice training, a...  transvoice  \n",
       "993                                                     transvoice  \n",
       "994                                                     transvoice  \n",
       "995                                                     transvoice  \n",
       "996                                                     transvoice  \n",
       "\n",
       "[997 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_ = scrape_reddit('transvoice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Combining\n",
    "# Identifying the y Values\n",
    "df_SubredditDrama['target_sub'] = 1\n",
    "df_AskReddit['target_sub']      = 0\n",
    "\n",
    "# Concatination of the two subreddits\n",
    "df_compare = pd.concat([df_SubredditDrama.drop('subreddit', axis=1),\n",
    "                        df_AskReddit.drop('subreddit', axis=1)])\n",
    "\n",
    "# Filling Nulls\n",
    "df_compare.fillna('', inplace=True)\n",
    "\n",
    "# Combining the title and selftext columns\n",
    "df_compare['all_text'] = df_compare['title'] + ' ' + df_compare['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "df_compare.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name         0\n",
       "Title        0\n",
       "Selftext     0\n",
       "Subreddit    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fds.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the title and selftext columns\n",
    "df_compare['all_text'] = df_compare['title'] + ' ' + df_compare['selftext']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most correlated to Target subreddit???\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user         0.352601\n",
       "http         0.313004\n",
       "com          0.312413\n",
       "reddit       0.304718\n",
       "comment      0.302215\n",
       "www          0.275697\n",
       "drama        0.271083\n",
       "thread       0.241970\n",
       "post         0.240567\n",
       "argument     0.191756\n",
       "debate       0.191575\n",
       "op           0.185547\n",
       "context      0.178263\n",
       "subreddit    0.177042\n",
       "source       0.176399\n",
       "mod          0.174861\n",
       "share        0.173518\n",
       "medium       0.173511\n",
       "amp          0.172538\n",
       "Name: target_sub, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing the correlations to the two data frames.\n",
    "# 1 = represents coming from lesbians subreddit.\n",
    "# 0 = represents coming from incels subreddit.\n",
    "df_corrs = df_words.corr().sort_values(['target_sub'])['target_sub']\n",
    "print(\"Most correlated to Target subreddit???\")\n",
    "df_corrs.tail(20)[18::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value = \n",
    "print(test_value)\n",
    "def fun_(val_in):\n",
    "    return val_out\n",
    "funct_map_mins(test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incels.drop(['is_lesbians', 'target_sub'], \n",
    "               axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0     Goodbye, guys. I leave you all with this song.   \n",
      "1  Found a false flag IncelTear/AHS user. Watch o...   \n",
      "2  First day of kindergarten tomorrow? Any advice...   \n",
      "3                    Joblless Female Geting Rejected   \n",
      "4           “The past is the past guys”. Get over it   \n",
      "\n",
      "                                            selftext  subreddit  is_lesbians  \\\n",
      "0  I was going to do this on a throwaway and just...  Braincels            0   \n",
      "1                                                NaN  Braincels            0   \n",
      "2  Sup guys I'm a 4 year old incel (got rejected ...  Braincels            0   \n",
      "3                                                NaN  Braincels            0   \n",
      "4                                                NaN  Braincels            0   \n",
      "\n",
      "   target_sub  \n",
      "0           1  \n",
      "1           1  \n",
      "2           1  \n",
      "3           1  \n",
      "4           1  \n",
      "Most correlated to Target subreddit?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "amp           0.172538\n",
       "medium        0.173511\n",
       "share         0.173518\n",
       "mod           0.174861\n",
       "source        0.176399\n",
       "subreddit     0.177042\n",
       "context       0.178263\n",
       "op            0.185547\n",
       "debate        0.191575\n",
       "argument      0.191756\n",
       "post          0.240567\n",
       "thread        0.241970\n",
       "drama         0.271083\n",
       "www           0.275697\n",
       "comment       0.302215\n",
       "reddit        0.304718\n",
       "com           0.312413\n",
       "http          0.313004\n",
       "user          0.352601\n",
       "target_sub    1.000000\n",
       "Name: target_sub, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_value = df_incels\n",
    "print(test_value.head())\n",
    "def fun_identify_jargon(df_in):\n",
    "    df_in['target_sub']        = 1\n",
    "    df_AskReddit['target_sub'] = 0\n",
    "\n",
    "# Prepping Concatted Dataframe\n",
    "    # Concatination of the two subreddits\n",
    "#    df_compare = pd.concat([df_SubredditDrama.drop('subreddit', axis=1),\n",
    "#        df_AskReddit.drop('subreddit', axis=1)])\n",
    "#    df_compare.fillna('', inplace=True)\n",
    "#    df_compare['all_text'] = df_compare['title'\n",
    "#        ] + ' ' + df_compare['selftext']\n",
    "#    df_compare.reset_index(inplace=True)\n",
    "#\n",
    "#    # Creating Cvec DataFrame of both forums\n",
    "#    df_words = pd.DataFrame(cvec.fit_transform(df_compare['all_text']\n",
    "#        ).todense(), columns=cvec.get_feature_names())\n",
    "#    df_words['target_sub'] = df_compare['target_sub']\n",
    "\n",
    "# Listing the correlations to the two data frames.\n",
    "#    df_corrs = df_words.corr().sort_values(['target_sub'])['target_sub']\n",
    "#    print(\"Most correlated to Target subreddit?\")\n",
    "#    return df_corrs.tail(20)#[18::-1]\n",
    "fun_identify_jargon(test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the correlations to the two data frames.\n",
    "# 1 = represents coming from lesbians subreddit.\n",
    "# 0 = represents coming from incels subreddit.\n",
    "df_corrs = df_words.corr().sort_values(['is_lesbians'])['is_lesbians']\n",
    "print(\"Most correlated to Lesbians subreddit\")\n",
    "df_corrs.tail(20)[18::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most correlated to Incels subreddit\")\n",
    "df_corrs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Jargon\n",
    "\n",
    "# Note: Cut out words used in AskReddit Sub? (If \"girl\" has been used once or more in Ask, \n",
    "    # then cut it out of the Question DF)\n",
    "# Identifying the y Values\n",
    "['is_target'] = 1\n",
    "Ask['is_target'] = 0\n",
    "\n",
    "# Concatination of the two subreddits\n",
    "df_jargon = pd.concat([targets.drop('subreddit', axis=1),\n",
    "                        Ask.drop('subreddit', axis=1)],\n",
    "                        sort=True)\n",
    "\n",
    "# Filling Nulls\n",
    "df_jargon.fillna('', inplace=True)\n",
    "\n",
    "# Combining the title and selftext columns\n",
    "df_jargon['all_text'] = df_jargon['title'] + ' ' + df_jargon['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "df_jargon.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Creating Cvec DataFrame of both forums\n",
    "df_words = pd.DataFrame(cvec.fit_transform(df_jargon['all_text']).todense(), \n",
    "                        columns=cvec.get_feature_names())\n",
    "\n",
    "# Inserting the target column\n",
    "df_words['is_target'] = df_jargon['is_target']\n",
    "# Listing the correlations to the two data frames.\n",
    "# 1 = represents coming from lesbians subreddit.\n",
    "# 0 = represents coming from incels subreddit.\n",
    "df_corrs = df_words.corr().sort_values(['is_target'])['is_target']\n",
    "print(\"Most correlated to target subreddit\")\n",
    "df_corrs.tail(20)[18::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_value = df_incels\n",
    "print(test_value.head())\n",
    "def fun_identify_jargon(df_in):\n",
    "    df_in['target_sub']        = 1\n",
    "    df_AskReddit['target_sub'] = 0\n",
    "\n",
    "# Prepping Concatted Dataframe\n",
    "    # Concatination of the two subreddits\n",
    "#    df_compare = pd.concat([df_SubredditDrama.drop('subreddit', axis=1),\n",
    "#        df_AskReddit.drop('subreddit', axis=1)])\n",
    "#    df_compare.fillna('', inplace=True)\n",
    "#    df_compare['all_text'] = df_compare['title'\n",
    "#        ] + ' ' + df_compare['selftext']\n",
    "#    df_compare.reset_index(inplace=True)\n",
    "#\n",
    "#    # Creating Cvec DataFrame of both forums\n",
    "#    df_words = pd.DataFrame(cvec.fit_transform(df_compare['all_text']\n",
    "#        ).todense(), columns=cvec.get_feature_names())\n",
    "#    df_words['target_sub'] = df_compare['target_sub']\n",
    "\n",
    "# Listing the correlations to the two data frames.\n",
    "#    df_corrs = df_words.corr().sort_values(['target_sub'])['target_sub']\n",
    "#    print(\"Most correlated to Target subreddit?\")\n",
    "#    return df_corrs.tail(20)#[18::-1]\n",
    "fun_identify_jargon(test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Jargon\n",
    "\n",
    "# Note: Cut out words used in AskReddit Sub? (If \"girl\" has been used once or more in Ask, \n",
    "    # then cut it out of the Question DF)\n",
    "\n",
    "# Concatination of the two subreddits\n",
    "\n",
    "# Combining the title and selftext columns\n",
    "df_jargon['all_text'] = df_jargon['title'] + ' ' + df_jargon['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating Cvec DataFrame of both forums\n",
    "df_words = pd.DataFrame(cvec.fit_transform(df_jargon['all_text']).todense(), \n",
    "                        columns=cvec.get_feature_names())\n",
    "\n",
    "# Inserting the target column\n",
    "df_words['is_target'] = df_jargon['is_target']\n",
    "# Listing the correlations to the two data frames.\n",
    "# 1 = represents coming from lesbians subreddit.\n",
    "# 0 = represents coming from incels subreddit.\n",
    "df_corrs = df_words.corr().sort_values(['is_target'])['is_target']\n",
    "print(\"Most correlated to target subreddit\")\n",
    "df_corrs.tail(20)[18::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_value = df_incels\n",
    "print(test_value.head())\n",
    "def fun_identify_jargon(df_in):\n",
    "    df_in['target_sub']        = 1\n",
    "    df_AskReddit['target_sub'] = 0\n",
    "\n",
    "# Prepping Concatted Dataframe\n",
    "    # Concatination of the two subreddits\n",
    "#    df_compare = pd.concat([df_SubredditDrama.drop('subreddit', axis=1),\n",
    "#        df_AskReddit.drop('subreddit', axis=1)])\n",
    "#    df_compare.fillna('', inplace=True)\n",
    "#    df_compare['all_text'] = df_compare['title'\n",
    "#        ] + ' ' + df_compare['selftext']\n",
    "#    df_compare.reset_index(inplace=True)\n",
    "#\n",
    "#    # Creating Cvec DataFrame of both forums\n",
    "#    df_words = pd.DataFrame(cvec.fit_transform(df_compare['all_text']\n",
    "#        ).todense(), columns=cvec.get_feature_names())\n",
    "#    df_words['target_sub'] = df_compare['target_sub']\n",
    "\n",
    "# Listing the correlations to the two data frames.\n",
    "#    df_corrs = df_words.corr().sort_values(['target_sub'])['target_sub']\n",
    "#    print(\"Most correlated to Target subreddit?\")\n",
    "#    return df_corrs.tail(20)#[18::-1]\n",
    "fun_identify_jargon(test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words more correlated to one vs the other\n",
    "\n",
    "# Identifying the y Values\n",
    "df_lesbians['is_lesbians'] = 1\n",
    "df_incels['is_lesbians'] = 0\n",
    "\n",
    "# Concatination of the two subreddits\n",
    "les_or_inc = pd.concat([df_lesbians.drop('subreddit', axis=1),\n",
    "                        df_incels.drop('subreddit', axis=1)])\n",
    "\n",
    "# Filling Nulls\n",
    "les_or_inc.fillna('', inplace=True)\n",
    "\n",
    "# Combining the title and selftext columns\n",
    "les_or_inc['all_text'] = les_or_inc['title'] + ' ' + les_or_inc['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "les_or_inc.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Creating Cvec DataFrame of both forums\n",
    "df_words = pd.DataFrame(cvec.fit_transform(les_or_inc['all_text']).todense(), \n",
    "                        columns=cvec.get_feature_names())\n",
    "\n",
    "# Inserting the target column\n",
    "df_words['is_lesbians'] = les_or_inc['is_lesbians']\n",
    "\n",
    "\n",
    "\n",
    "# Listing the correlations to the two data frames.\n",
    "# 1 = represents coming from lesbians subreddit.\n",
    "# 0 = represents coming from incels subreddit.\n",
    "df_corrs = df_words.corr().sort_values(['is_lesbians'])['is_lesbians']\n",
    "print(\"Most correlated to Lesbians subreddit\")\n",
    "df_corrs.tail(20)[18::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the y Values\n",
    "df_SubredditDrama['target_sub'] = 1\n",
    "df_AskReddit['target_sub']      = 0\n",
    "\n",
    "# Concatination of the two subreddits\n",
    "df_compare = pd.concat([df_SubredditDrama.drop('subreddit', axis=1),\n",
    "                        df_AskReddit.drop('subreddit', axis=1)])\n",
    "\n",
    "# Filling Nulls\n",
    "df_compare.fillna('', inplace=True)\n",
    "\n",
    "# Combining the title and selftext columns\n",
    "df_compare['all_text'] = df_compare['title'] + ' ' + df_compare['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "df_compare.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Cvec DataFrame of both forums\n",
    "df_words = pd.DataFrame(cvec.fit_transform(df_compare['all_text']).todense(), \n",
    "                        columns=cvec.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
