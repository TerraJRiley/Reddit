{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "# Executive Summary\n",
    "How many times have you opened up a browser for a random subreddit only to find that it wasn't the random subreddit you were looking for?  We've all been there.  Furthermore, what about when you wonder \"golly, just how similar are different subreddits that are focused one concept but from entirely different points of view?\"  Well, we hear you.  We've scrapped data from two active subreddits which focus around sexuality and using them build a model that's able to detect if it's one subreddit or the other with over an 80% certainty.  Furthermore, if future exploritory data analysis, we hope to one day be able to talk about the defining features of each subculter that's being represented by these subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function that scrapes a subreddit and turns it into a pandas dataframe.\n",
    "Followed by it being used for the actuallesbians, Braincels and Trufemcels subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit(the_subreddit, pages = 40):\n",
    "    all_posts = []\n",
    "    first_url = 'http://www.reddit.com/r/' + the_subreddit + '.json'\n",
    "    url = first_url\n",
    "    list_of_df = []\n",
    "    \n",
    "    #Putting in a get check, for happy sanity reasons:\n",
    "    quick_check = requests.get(first_url, headers = {'User-agent':'Electronic Goddess'})\n",
    "    if int(str(quick_check)[11:14]) == 200:\n",
    "        print(\"Get request successful.\")\n",
    "        time.sleep(3)\n",
    "        print(\"Initiating Scrape...\")\n",
    "    else:\n",
    "        print(\"Get request not 200, instead recieved:\" + str(quick_check))\n",
    "        return\n",
    "    \n",
    "    #Now for the actual Scraping:\n",
    "    for round in range(pages):\n",
    "        try:\n",
    "            res = requests.get(url, headers = {'User-agent':'Electronic Goddess'})\n",
    "            data = res.json()\n",
    "            list_of_posts = data['data']['children']\n",
    "            all_posts = all_posts + list_of_posts\n",
    "            after = data['data']['after']\n",
    "            url = first_url +'?after=' + after\n",
    "            print('Current After:' + after,'Round: '+ str(round + 1))\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            print('Limit likely hit.  Returning available posts.')\n",
    "            break\n",
    "#        return all_posts # This can be un-commented out incase I want the straight forward raw scrape\n",
    "\n",
    "    #Formats the parts we care about into a list of dictionaries that'll become the dataframe\n",
    "    for i in range(len(all_posts)):\n",
    "        index_dictionary = {\n",
    "                'title' : all_posts[i]['data']['title'],\n",
    "                'selftext': all_posts[i]['data']['selftext'],\n",
    "                'subreddit' : all_posts[i]['data']['subreddit']\n",
    "            }\n",
    "        list_of_df.append(index_dictionary)\n",
    "    return pd.DataFrame(list_of_df, columns = ['title','selftext','subreddit'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the scrappings that we'll be actually using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get request successful.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-485789d82fd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_lesbians\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_reddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'actuallesbians'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_incels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_reddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'braincels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-01363a059796>\u001b[0m in \u001b[0;36mscrape_reddit\u001b[0;34m(the_subreddit, pages)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquick_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Get request successful.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initiating Scrape...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_lesbians = scrape_reddit('actuallesbians')\n",
    "df_incels = scrape_reddit('braincels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Subreddits to check out if there is the opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_femcels = scrape_reddit('Trufemcels')\n",
    "#df_gaybros = scrape_reddit('gaybros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Saved and available to be loaded from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "# Export to csv (Commented out to avoid re-saving errors)\n",
    "#df_lesbians.to_csv('actuallesbians_9_9_400', index=False)\n",
    "#df_incels.to_csv('braincels_9_9_400', index=False)\n",
    "#df_femcels.to_csv('trufemcels_9_9_1000', index=False)\n",
    "#df_gaybros.to_csv('gaybros_9_10_540', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from CSV\n",
    "df_lesbians = pd.read_csv('./actuallesbians_9_9_400')\n",
    "df_incels = pd.read_csv('./braincels_9_9_400')\n",
    "#df_femcels = pd.read_csv('./trufemcels_9_9_1000')\n",
    "#df_gaybros = pd.read_csv('./gaybros_9_10_540')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploritory Data Analysis\n",
    "    What are the most used words for each subreddit?\n",
    "    Are the most used words jargon?\n",
    "    How much text on average do the subredditors post?\n",
    "    How many of the posts are pictures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost No EDA has be done at this time in order to expidite the process of getting this project finished.  That which was mentioned during the presentation was from memory prior to the loss of my previous work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Using CountVectorizer &/or TF-IDF to generate features from the post text and title of posts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lesbians['selftext'].apply(text_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiations of the tokenizer, lemmatizer and Count Vectorizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = tokenizer.tokenize,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = 'english') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining and altering the dataframes to be modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the y Values\n",
    "df_lesbians['is_lesbians'] = 1\n",
    "df_incels['is_lesbians'] = 0\n",
    "\n",
    "# Concatination\n",
    "les_or_inc = pd.concat([df_lesbians.drop('subreddit',axis=1),df_incels.drop('subreddit', axis=1)])\n",
    "\n",
    "# Filling Nulls\n",
    "les_or_inc.fillna('', inplace=True)\n",
    "\n",
    "# Combining the title and selftext columns for easier Count Vectorization\n",
    "les_or_inc['all_text'] = les_or_inc['title'] + ' ' + les_or_inc['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "les_or_inc.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the X,y, as well as the tests and trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "X = les_or_inc['all_text']\n",
    "y = les_or_inc['is_lesbians']\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    random_state=76)\n",
    "# Count Vectorizing the train and test X's while fitting the Training X\n",
    "X_train = pd.DataFrame(cvec.fit_transform(X_train).todense(), columns=cvec.get_feature_names())\n",
    "X_test = pd.DataFrame(cvec.transform(X_test).todense(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "source": [
    "The baseline accuracy for this model is about 50% because one could simply guess 1 or 0 for all of the rows and get 50% correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and testing MultinomialNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9513513513513514\n",
      "Test: 0.8704453441295547\n"
     ]
    }
   ],
   "source": [
    "multi_model = MultinomialNB().fit(X_train,y_train)\n",
    "\n",
    "print(\"Train:\", multi_model.score(X_train,y_train))\n",
    "\n",
    "print(\"Test:\", multi_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and testing RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9932432432432432\n",
      "Test: 0.8218623481781376\n"
     ]
    }
   ],
   "source": [
    "rando_forest = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(\"Train:\", rando_forest.score(X_train,y_train))\n",
    "\n",
    "print(\"Test:\", rando_forest.score(X_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
