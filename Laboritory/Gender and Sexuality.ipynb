{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "# Executive Summary\n",
    "How many times have you opened up a browser for a random subreddit only to find that it wasn't the random subreddit you were looking for?  We've all been there.  Furthermore, what about when you wonder \"golly, just how similar are different subreddits that are focused one concept but from entirely different points of view?\"  Well, we hear you.  We've scrapped data from two active subreddits which focus around sexuality and using them build a model that's able to detect if it's one subreddit or the other with over an 80% certainty.  Furthermore, if future exploritory data analysis, we hope to one day be able to talk about the defining features of each subculter that's being represented by these subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifting over to the analysis of multiple Reddits based on gender and sexuality.\n",
    "\n",
    "Female, Straight:\n",
    "Manosphere:\n",
    " - seduction\n",
    " - pickup\n",
    " - men\n",
    " - OneY\n",
    "\n",
    "Queer:\n",
    " - FEMM\n",
    " - FemmeThoughts\n",
    " - FIREyFemmes\n",
    " - MaleFemme\n",
    " - lgbt\n",
    " - ainbow\n",
    " - AskLGBT\n",
    " - happentobegay\n",
    " - glbt\n",
    " - gay\n",
    " - gayrights\n",
    " - lgbtsex\n",
    " - queer\n",
    " - asexual\n",
    " - asexuality\n",
    " - bisexual\n",
    " - pansexual\n",
    " - q4q\n",
    " - actuallesbians\n",
    " - lgbtcirclejerk\n",
    " - QueerTransmen\n",
    " - asktransgender\n",
    " - androgynoushotties\n",
    "\n",
    "Queer Geeks/Nerds:\n",
    " - gaygeek\n",
    " - GaymersGoneMild\n",
    " - GirlGamers\n",
    "\n",
    "### To be obtained\n",
    "\n",
    "Feminism:\n",
    " - AskFeminists\n",
    " - antifeminists\n",
    " - RadicalFeminism\n",
    " - DebateFeminism\n",
    " - GenderCritical\n",
    " - Feminism\n",
    "Femmes\n",
    " - FEMM\n",
    " - Femme\n",
    " - FemmeThoughts\n",
    " - FIREyFemmes\n",
    " - MaleFemme\n",
    "Butch\n",
    " - LesbianActually\n",
    " - butchlesbians\n",
    " \n",
    "Mens\n",
    " - AskMen\n",
    " - Manosphere\n",
    "QPOC\n",
    " - QueerWomenOfColor\n",
    " - gaypoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Broken subreddits:\n",
    "    - puascience\n",
    "    - pua\n",
    "    - lgb\n",
    "    - LGBTVent\n",
    "    - QPOC\n",
    "    - TransHack\n",
    "    - QueerFashionAdvice\n",
    "    - TwoXChromosome\n",
    "    - lesbians\n",
    "    \n",
    "Redo: gaymers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Class(object):\n",
    "    \"Discription of the class\"\n",
    "\n",
    "    def __init__(self, required_arg, stated_arg = 0.0): # Happens at initialization\n",
    "        \"\"\"Happens at initialization.  \n",
    "        This appears in the class description.\"\"\"\n",
    "        self.value_1 = required_arg                     # Required to be stated value\n",
    "        self.value_2 = stated_arg                       # Optional because stated argument\n",
    "        return\n",
    "\n",
    "    def __str__(self):\n",
    "        'Prints description of the class.'\n",
    "        return\n",
    "\n",
    "    \n",
    "    def regular_function(self, arg):\n",
    "        \"Appears in function description\"\n",
    "        if arg > self.balance:\n",
    "            raise RuntimeError('Arg > balance.')     # Error check\n",
    "        self.value_2 -= arg                             # Resaves self.value_2\n",
    "        return self.value_2                             # Returns/states value_2\n",
    "        \n",
    "    def __del__(self):\n",
    "        'Destructor; deletes an object.'\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "pull_date = datetime.fromtimestamp(time()).strftime('%m_%d_%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pull_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class subreddit(object):\n",
    "    \n",
    "    def generate_from_api(self, the_subreddit, pages = 40):\n",
    "        all_posts = []\n",
    "        first_url = 'http://www.reddit.com/r/' + the_subreddit + '.json'\n",
    "        url = first_url\n",
    "        list_of_df = []\n",
    "\n",
    "        for round in range(pages):\n",
    "            try:\n",
    "                res = requests.get(url, headers = {'User-agent':'Electronic Goddess'})\n",
    "                data = res.json()\n",
    "                list_of_posts = data['data']['children']\n",
    "                all_posts = all_posts + list_of_posts\n",
    "                after = data['data']['after']\n",
    "                url = first_url +'?after=' + after\n",
    "                print('Current After:' + after,'Round: '+ str(round + 1))\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                print('Limit likely hit.  Returning available posts.')\n",
    "                break\n",
    "\n",
    "        # Formats the parts we care about into a list of dictionaries that'll become the dataframe\n",
    "        for i in range(len(all_posts)):\n",
    "            index_dictionary = {\n",
    "                    'title' : all_posts[i]['data']['title'],\n",
    "                    'selftext': all_posts[i]['data']['selftext'],\n",
    "                    'subreddit' : all_posts[i]['data']['subreddit']\n",
    "                }\n",
    "            list_of_df.append(index_dictionary)\n",
    "        df = pd.DataFrame(list_of_df, columns = ['title','selftext','subreddit'])\n",
    "        return pd.DataFrame(list_of_df, columns = ['title','selftext','subreddit'])\n",
    "    \n",
    "    def __init__(self, reddit_title, from_archive = True, path = './Data/'):\n",
    "        if   from_archive == True:\n",
    "            self.df = pd.read_csv(path + str(reddit_title))\n",
    "        elif from_archive == False: \n",
    "            self.df = generate_from_api(reddit_title, pages = 40)\n",
    "            pull_date = datetime.fromtimestamp(time()).strftime('%m_%d_%Y')\n",
    "        else:\n",
    "            raise Error(\"WTF, load the df from somewhere.  It's literally everything\")\n",
    "        return\n",
    "    \n",
    "    def pull_from_csv(self, path):\n",
    "        df = pd.DataFrame.read_csv(path)\n",
    "        return pd.DataFrame.read_csv(path)\n",
    "    #def save(self):\n",
    "    #    self.df.to_read('./Data/) #path + redditname + pulldate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions & attributes:\n",
    "    cvec\n",
    "     only text\n",
    "     only title\n",
    "     both\n",
    "    hvec\n",
    "     1/1/2\n",
    "    TF-IDF\n",
    "    # Memes & Images\n",
    "    most used words\n",
    "    Jargon\n",
    "    n-grams\n",
    "    Sentiment\n",
    "    .save\n",
    "        #Can we auto-create a data file if it doesn't already exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lezies = subreddit('AskLGBT_10_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lezies.df.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class subreddit(object):\n",
    "    #df = \"Generate either with .generate_from_api() or .pull_from_csv()\"\n",
    "    \n",
    "    def generate_from_api(self, the_subreddit, pages = 40):\n",
    "        all_posts = []\n",
    "        first_url = 'http://www.reddit.com/r/' + the_subreddit + '.json'\n",
    "        url = first_url\n",
    "        list_of_df = []\n",
    "\n",
    "        for round in range(pages):\n",
    "            try:\n",
    "                res = requests.get(url, headers = {'User-agent':'Electronic Goddess'})\n",
    "                data = res.json()\n",
    "                list_of_posts = data['data']['children']\n",
    "                all_posts = all_posts + list_of_posts\n",
    "                after = data['data']['after']\n",
    "                url = first_url +'?after=' + after\n",
    "                print('Current After:' + after,'Round: '+ str(round + 1))\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                print('Limit likely hit.  Returning available posts.')\n",
    "                break\n",
    "\n",
    "        # Formats the parts we care about into a list of dictionaries that'll become the dataframe\n",
    "        for i in range(len(all_posts)):\n",
    "            index_dictionary = {\n",
    "                    'title' : all_posts[i]['data']['title'],\n",
    "                    'selftext': all_posts[i]['data']['selftext'],\n",
    "                    'subreddit' : all_posts[i]['data']['subreddit']\n",
    "                }\n",
    "            list_of_df.append(index_dictionary)\n",
    "        df = pd.DataFrame(list_of_df, columns = ['title','selftext','subreddit'])\n",
    "        return pd.DataFrame(list_of_df, columns = ['title','selftext','subreddit'])\n",
    "        \n",
    "    def pull_from_csv(path):\n",
    "        df = pd.DataFrame.from_csv(path)\n",
    "        return pd.DataFrame.from_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Human = Customer('Human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other\n"
     ]
    }
   ],
   "source": [
    "Human.deposit(2)\n",
    "print('other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function that scrapes a subreddit and turns it into a pandas dataframe.\n",
    "Followed by it being used for the actuallesbians, Braincels and Trufemcels subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = subreddit#.pull_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'subreddit' has no attribute 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7184ab9d30ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'subreddit' has no attribute 'df'"
     ]
    }
   ],
   "source": [
    "test.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pearl/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel/__main__.py:36: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "/Users/pearl/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel/__main__.py:37: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Reminder: please only post selfies/pictures of yourself in the Wednesday and Saturday mega threads</th>\n",
       "      <td>Lately there's been quite a few selfies/self p...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sunday Daily Chat Thread</th>\n",
       "      <td>Welcome to the daily chat thread! These are a ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>üòçüòç im gay</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Me as a girlfriend</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I can feel it in me bones</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I know these are advertised at best friends but all I could think about is how cute it would be to wear with a girlfriend</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If you‚Äôre from the west and just saw India decriminalize lgbt sex</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Girl: *does literally anything*</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kittens will also do the trick.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yup... I should have caught that.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lmaooo I made this</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is this you too?</th>\n",
       "      <td>Is it just me? Or do some of you out there als...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yassss</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Why tho</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Holy Mary Mother of God, Eva Green</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hmmmmm.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never stop running your hand through your hair</th>\n",
       "      <td>nothing knackers my banger on a barbie more th...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Holy shit you guys...</th>\n",
       "      <td>Apparently there's a TV show called All Girls ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I feel called out (minor OITNB s6 spoilers, maybe?)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>When the girl you like is sending you mixed messages</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Just a silly little thing I realized</th>\n",
       "      <td>I recently realized something. Growing up I al...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>There goes my HeroüôÉ</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wishing I had someone to watch the sunrise with.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"Adventure Time\": just Gal Pals liking each other a lot!</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ladies of the subreddit, I present...</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recorded before I was born, but Joan Jett is the hottest woman I've ever seen</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Look at these little potatoes üòªüíú</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I think Aubrey Plaza just made me gayer.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All I want is pizza and a cute girl dancing next to me tbh.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Love all you wholesome women</th>\n",
       "      <td>Ok, is anyone gonna talk about how cute and wh...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How do I figure out my feelings for girls are real...</th>\n",
       "      <td>How do I figure out my feelings for girls are ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>While you were closeted</th>\n",
       "      <td>while you were closeted, did you ever hate or ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I was 0/7 as a Child. Thanks Abstinence Only Education!</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Even if it's a kid's show, this is still my favourite lesbian couple.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Desiring a closeted religious bi girl: a heartache in vignettes</th>\n",
       "      <td>you breathe my name out like a prayer with the...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saw a commercial for this movie and i'm left thinking ... \"But is it gay?\"</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You Guys Are Awesome!</th>\n",
       "      <td>Since coming to terms with my sexuality I feel...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good Seattle places for queer women?</th>\n",
       "      <td>I've been a Seattleite all my life and I don't...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How do you feel about lesbians who sexually objectify women?</th>\n",
       "      <td>I was watching Dead Sara's music video for \"Mo...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How can I tell if someone is into me?</th>\n",
       "      <td>I'm back with another hypothetical! Add fast a...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sapphic pendants?</th>\n",
       "      <td>Fellow leslies! \\n\\nI‚Äôm a rather femme gal and...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my friend needs help</th>\n",
       "      <td>hello my friend is transitioning and she reall...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I really love her</th>\n",
       "      <td>So I already posted about this topic before bu...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Juggler</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Potential First Girlfriend</th>\n",
       "      <td>I started talking to this girl on Tinder a few...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conflicting information</th>\n",
       "      <td>I'm rather inexperienced with pretty much all ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFF or Girlfriend?</th>\n",
       "      <td>Hi all, I need some advice!\\n\\nSo my best frie...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uhm... I just fell in loveüôà</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Even Sappho fell in love with a straight girl</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strongly considering losing my virginity to a woman from Tinder, thoughts?</th>\n",
       "      <td>I‚Äôm a 25 year old girl who hasn‚Äôt had sex yet ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Any ideas on how to help her smile ...</th>\n",
       "      <td>Hey guys, I have posted on here before about m...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dating is feeling like a huge bummer</th>\n",
       "      <td>I'm just not finding anyone I connect with and...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Halo!</th>\n",
       "      <td>I know this may not be appropriate and if not ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asking Coworker to Lunch..</th>\n",
       "      <td>How do you as a coworker to lunch without it b...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Was trying to explain to my mom I wanted to wear a suit to a wedding</th>\n",
       "      <td>Me: well it‚Äôs in late October so I don‚Äôt want ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Looking for some advice</th>\n",
       "      <td>So I started chatting with this girl on OkCupi...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lesbian Youtubers</th>\n",
       "      <td>Hey all! I want to subscribe to more lesbian Y...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Had to share</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Halsey Plz</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fairly new to the board and wanted to say hi!</th>\n",
       "      <td>Hey fellow Raptors! I have been spending more ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>993 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             selftext  \\\n",
       "title                                                                                                   \n",
       "Reminder: please only post selfies/pictures of ...  Lately there's been quite a few selfies/self p...   \n",
       "Sunday Daily Chat Thread                            Welcome to the daily chat thread! These are a ...   \n",
       "üòçüòç im gay                                                                                         NaN   \n",
       "Me as a girlfriend                                                                                NaN   \n",
       "I can feel it in me bones                                                                         NaN   \n",
       "I know these are advertised at best friends but...                                                NaN   \n",
       "If you‚Äôre from the west and just saw India decr...                                                NaN   \n",
       "Girl: *does literally anything*                                                                   NaN   \n",
       "Kittens will also do the trick.                                                                   NaN   \n",
       "Yup... I should have caught that.                                                                 NaN   \n",
       "Lmaooo I made this                                                                                NaN   \n",
       "Is this you too?                                    Is it just me? Or do some of you out there als...   \n",
       "yassss                                                                                            NaN   \n",
       "Why tho                                                                                           NaN   \n",
       "Holy Mary Mother of God, Eva Green                                                                NaN   \n",
       "Hmmmmm.                                                                                           NaN   \n",
       "never stop running your hand through your hair      nothing knackers my banger on a barbie more th...   \n",
       "Holy shit you guys...                               Apparently there's a TV show called All Girls ...   \n",
       "I feel called out (minor OITNB s6 spoilers, may...                                                NaN   \n",
       "When the girl you like is sending you mixed mes...                                                NaN   \n",
       "Just a silly little thing I realized                I recently realized something. Growing up I al...   \n",
       "There goes my HeroüôÉ                                                                               NaN   \n",
       "Wishing I had someone to watch the sunrise with.                                                  NaN   \n",
       "\"Adventure Time\": just Gal Pals liking each oth...                                                NaN   \n",
       "Ladies of the subreddit, I present...                                                             NaN   \n",
       "Recorded before I was born, but Joan Jett is th...                                                NaN   \n",
       "Look at these little potatoes üòªüíú                                                                  NaN   \n",
       "I think Aubrey Plaza just made me gayer.                                                          NaN   \n",
       "All I want is pizza and a cute girl dancing nex...                                                NaN   \n",
       "Love all you wholesome women                        Ok, is anyone gonna talk about how cute and wh...   \n",
       "...                                                                                               ...   \n",
       "How do I figure out my feelings for girls are r...  How do I figure out my feelings for girls are ...   \n",
       "While you were closeted                             while you were closeted, did you ever hate or ...   \n",
       "I was 0/7 as a Child. Thanks Abstinence Only Ed...                                                NaN   \n",
       "Even if it's a kid's show, this is still my fav...                                                NaN   \n",
       "Desiring a closeted religious bi girl: a hearta...  you breathe my name out like a prayer with the...   \n",
       "Saw a commercial for this movie and i'm left th...                                                NaN   \n",
       "You Guys Are Awesome!                               Since coming to terms with my sexuality I feel...   \n",
       "Good Seattle places for queer women?                I've been a Seattleite all my life and I don't...   \n",
       "How do you feel about lesbians who sexually obj...  I was watching Dead Sara's music video for \"Mo...   \n",
       "How can I tell if someone is into me?               I'm back with another hypothetical! Add fast a...   \n",
       "Sapphic pendants?                                   Fellow leslies! \\n\\nI‚Äôm a rather femme gal and...   \n",
       "my friend needs help                                hello my friend is transitioning and she reall...   \n",
       "I really love her                                   So I already posted about this topic before bu...   \n",
       "Juggler                                                                                           NaN   \n",
       "Potential First Girlfriend                          I started talking to this girl on Tinder a few...   \n",
       "Conflicting information                             I'm rather inexperienced with pretty much all ...   \n",
       "BFF or Girlfriend?                                  Hi all, I need some advice!\\n\\nSo my best frie...   \n",
       "Uhm... I just fell in loveüôà                                                                       NaN   \n",
       "Even Sappho fell in love with a straight girl                                                     NaN   \n",
       "Strongly considering losing my virginity to a w...  I‚Äôm a 25 year old girl who hasn‚Äôt had sex yet ...   \n",
       "Any ideas on how to help her smile ...              Hey guys, I have posted on here before about m...   \n",
       "Dating is feeling like a huge bummer                I'm just not finding anyone I connect with and...   \n",
       "Halo!                                               I know this may not be appropriate and if not ...   \n",
       "Asking Coworker to Lunch..                          How do you as a coworker to lunch without it b...   \n",
       "Was trying to explain to my mom I wanted to wea...  Me: well it‚Äôs in late October so I don‚Äôt want ...   \n",
       "Looking for some advice                             So I started chatting with this girl on OkCupi...   \n",
       "Lesbian Youtubers                                   Hey all! I want to subscribe to more lesbian Y...   \n",
       "Had to share                                                                                      NaN   \n",
       "Halsey Plz                                                                                        NaN   \n",
       "Fairly new to the board and wanted to say hi!       Hey fellow Raptors! I have been spending more ...   \n",
       "\n",
       "                                                         subreddit  \n",
       "title                                                               \n",
       "Reminder: please only post selfies/pictures of ...  actuallesbians  \n",
       "Sunday Daily Chat Thread                            actuallesbians  \n",
       "üòçüòç im gay                                           actuallesbians  \n",
       "Me as a girlfriend                                  actuallesbians  \n",
       "I can feel it in me bones                           actuallesbians  \n",
       "I know these are advertised at best friends but...  actuallesbians  \n",
       "If you‚Äôre from the west and just saw India decr...  actuallesbians  \n",
       "Girl: *does literally anything*                     actuallesbians  \n",
       "Kittens will also do the trick.                     actuallesbians  \n",
       "Yup... I should have caught that.                   actuallesbians  \n",
       "Lmaooo I made this                                  actuallesbians  \n",
       "Is this you too?                                    actuallesbians  \n",
       "yassss                                              actuallesbians  \n",
       "Why tho                                             actuallesbians  \n",
       "Holy Mary Mother of God, Eva Green                  actuallesbians  \n",
       "Hmmmmm.                                             actuallesbians  \n",
       "never stop running your hand through your hair      actuallesbians  \n",
       "Holy shit you guys...                               actuallesbians  \n",
       "I feel called out (minor OITNB s6 spoilers, may...  actuallesbians  \n",
       "When the girl you like is sending you mixed mes...  actuallesbians  \n",
       "Just a silly little thing I realized                actuallesbians  \n",
       "There goes my HeroüôÉ                                 actuallesbians  \n",
       "Wishing I had someone to watch the sunrise with.    actuallesbians  \n",
       "\"Adventure Time\": just Gal Pals liking each oth...  actuallesbians  \n",
       "Ladies of the subreddit, I present...               actuallesbians  \n",
       "Recorded before I was born, but Joan Jett is th...  actuallesbians  \n",
       "Look at these little potatoes üòªüíú                    actuallesbians  \n",
       "I think Aubrey Plaza just made me gayer.            actuallesbians  \n",
       "All I want is pizza and a cute girl dancing nex...  actuallesbians  \n",
       "Love all you wholesome women                        actuallesbians  \n",
       "...                                                            ...  \n",
       "How do I figure out my feelings for girls are r...  actuallesbians  \n",
       "While you were closeted                             actuallesbians  \n",
       "I was 0/7 as a Child. Thanks Abstinence Only Ed...  actuallesbians  \n",
       "Even if it's a kid's show, this is still my fav...  actuallesbians  \n",
       "Desiring a closeted religious bi girl: a hearta...  actuallesbians  \n",
       "Saw a commercial for this movie and i'm left th...  actuallesbians  \n",
       "You Guys Are Awesome!                               actuallesbians  \n",
       "Good Seattle places for queer women?                actuallesbians  \n",
       "How do you feel about lesbians who sexually obj...  actuallesbians  \n",
       "How can I tell if someone is into me?               actuallesbians  \n",
       "Sapphic pendants?                                   actuallesbians  \n",
       "my friend needs help                                actuallesbians  \n",
       "I really love her                                   actuallesbians  \n",
       "Juggler                                             actuallesbians  \n",
       "Potential First Girlfriend                          actuallesbians  \n",
       "Conflicting information                             actuallesbians  \n",
       "BFF or Girlfriend?                                  actuallesbians  \n",
       "Uhm... I just fell in loveüôà                         actuallesbians  \n",
       "Even Sappho fell in love with a straight girl       actuallesbians  \n",
       "Strongly considering losing my virginity to a w...  actuallesbians  \n",
       "Any ideas on how to help her smile ...              actuallesbians  \n",
       "Dating is feeling like a huge bummer                actuallesbians  \n",
       "Halo!                                               actuallesbians  \n",
       "Asking Coworker to Lunch..                          actuallesbians  \n",
       "Was trying to explain to my mom I wanted to wea...  actuallesbians  \n",
       "Looking for some advice                             actuallesbians  \n",
       "Lesbian Youtubers                                   actuallesbians  \n",
       "Had to share                                        actuallesbians  \n",
       "Halsey Plz                                          actuallesbians  \n",
       "Fairly new to the board and wanted to say hi!       actuallesbians  \n",
       "\n",
       "[993 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.pull_from_csv('../Laboritory/Data/actuallesbians_9_9_400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(test.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class subreddit:\n",
    "def scrape_reddit(the_subreddit, pages = 40):\n",
    "    print(str(the_subreddit))\n",
    "    all_posts = []\n",
    "    first_url = 'http://www.reddit.com/r/' + the_subreddit + '.json'\n",
    "    url = first_url\n",
    "    list_of_df = []\n",
    "    \n",
    "    #Putting in a get check, for happy sanity reasons:\n",
    "    quick_check = requests.get(first_url, headers = {'User-agent':'Electronic Goddess'})\n",
    "    if int(str(quick_check)[11:14]) == 200:\n",
    "        print(\"Get request successful.\")\n",
    "        time.sleep(3)\n",
    "        print(\"Initiating Scrape...\")\n",
    "    else:\n",
    "        print(\"Get request not 200, instead recieved:\" + str(quick_check))\n",
    "        return\n",
    "    \n",
    "    #Now for the actual Scraping:\n",
    "    for round in range(pages):\n",
    "        try:\n",
    "            res = requests.get(url, headers = {'User-agent':'Electronic Goddess'})\n",
    "            data = res.json()\n",
    "            list_of_posts = data['data']['children']\n",
    "            all_posts = all_posts + list_of_posts\n",
    "            after = data['data']['after']\n",
    "            url = first_url +'?after=' + after\n",
    "            print('Current After:' + after,'Round: '+ str(round + 1))\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            print('Limit likely hit.  Returning available posts.')\n",
    "            break\n",
    "#        return all_posts # This can be un-commented out incase I want the straight forward raw scrape\n",
    "\n",
    "    #Formats the parts we care about into a list of dictionaries that'll become the dataframe\n",
    "    for i in range(len(all_posts)):\n",
    "        index_dictionary = {\n",
    "                'title' : all_posts[i]['data']['title'],\n",
    "                'selftext': all_posts[i]['data']['selftext'],\n",
    "                'subreddit' : all_posts[i]['data']['subreddit']\n",
    "            }\n",
    "        list_of_df.append(index_dictionary)\n",
    "    return pd.DataFrame(list_of_df, columns = ['title','selftext','subreddit'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the scrappings that we'll be actually using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get request successful.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-485789d82fd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_lesbians\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_reddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'actuallesbians'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_incels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_reddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'braincels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-01363a059796>\u001b[0m in \u001b[0;36mscrape_reddit\u001b[0;34m(the_subreddit, pages)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquick_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Get request successful.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initiating Scrape...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_lesbians = scrape_reddit('actuallesbians')\n",
    "df_incels = scrape_reddit('braincels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Subreddits to check out if there is the opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_femcels = scrape_reddit('Trufemcels')\n",
    "#df_gaybros = scrape_reddit('gaybros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Saved and available to be loaded from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "# Export to csv (Commented out to avoid re-saving errors)\n",
    "#df_lesbians.to_csv('actuallesbians_9_9_400', index=False)\n",
    "#df_incels.to_csv('braincels_9_9_400', index=False)\n",
    "#df_femcels.to_csv('trufemcels_9_9_1000', index=False)\n",
    "#df_gaybros.to_csv('gaybros_9_10_540', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from CSV\n",
    "df_lesbians = pd.read_csv('./actuallesbians_9_9_400')\n",
    "df_incels = pd.read_csv('./braincels_9_9_400')\n",
    "#df_femcels = pd.read_csv('./trufemcels_9_9_1000')\n",
    "#df_gaybros = pd.read_csv('./gaybros_9_10_540')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploritory Data Analysis\n",
    "    What are the most used words for each subreddit?\n",
    "    Are the most used words jargon?\n",
    "    How much text on average do the subredditors post?\n",
    "    How many of the posts are pictures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost No EDA has be done at this time in order to expidite the process of getting this project finished.  That which was mentioned during the presentation was from memory prior to the loss of my previous work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Using CountVectorizer &/or TF-IDF to generate features from the post text and title of posts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lesbians['selftext'].apply(text_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiations of the tokenizer, lemmatizer and Count Vectorizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = tokenizer.tokenize,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = 'english') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining and altering the dataframes to be modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the y Values\n",
    "df_lesbians['is_lesbians'] = 1\n",
    "df_incels['is_lesbians'] = 0\n",
    "\n",
    "# Concatination\n",
    "les_or_inc = pd.concat([df_lesbians.drop('subreddit',axis=1),df_incels.drop('subreddit', axis=1)])\n",
    "\n",
    "# Filling Nulls\n",
    "les_or_inc.fillna('', inplace=True)\n",
    "\n",
    "# Combining the title and selftext columns for easier Count Vectorization\n",
    "les_or_inc['all_text'] = les_or_inc['title'] + ' ' + les_or_inc['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "les_or_inc.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the X,y, as well as the tests and trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "X = les_or_inc['all_text']\n",
    "y = les_or_inc['is_lesbians']\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    random_state=76)\n",
    "# Count Vectorizing the train and test X's while fitting the Training X\n",
    "X_train = pd.DataFrame(cvec.fit_transform(X_train).todense(), columns=cvec.get_feature_names())\n",
    "X_test = pd.DataFrame(cvec.transform(X_test).todense(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "source": [
    "The baseline accuracy for this model is about 50% because one could simply guess 1 or 0 for all of the rows and get 50% correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and testing MultinomialNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9513513513513514\n",
      "Test: 0.8704453441295547\n"
     ]
    }
   ],
   "source": [
    "multi_model = MultinomialNB().fit(X_train,y_train)\n",
    "\n",
    "print(\"Train:\", multi_model.score(X_train,y_train))\n",
    "\n",
    "print(\"Test:\", multi_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and testing RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9932432432432432\n",
      "Test: 0.8218623481781376\n"
     ]
    }
   ],
   "source": [
    "rando_forest = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(\"Train:\", rando_forest.score(X_train,y_train))\n",
    "\n",
    "print(\"Test:\", rando_forest.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scrapped Code That I didn't want to delete, just incase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "source": [
    "cvec_lesbians['is_lesbians'] = 1\n",
    "cvec_incels['is_lesbians'] = 0\n",
    "\n",
    "les_or_inc = cvec_lesbians.add(cvec_incels, fill_value=0, axis=0)\n",
    "\n",
    "les_or_inc = les_or_inc.fillna(0).astype('int64')\n",
    "\n",
    "X = les_or_inc.drop(columns=['is_lesbians'])\n",
    "y = les_or_inc['is_lesbians']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    random_state=76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_lesbians' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e43e3c16ec04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_lesbians\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmysettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mXtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_lesbians' is not defined"
     ]
    }
   ],
   "source": [
    "df = df_lesbians['selftext'].map(text_prep)\n",
    "df = df.apply(text_prep)\n",
    "\n",
    "cvec = CountVectorizer(**mysettings).fit(Xtrain)\n",
    "Xtrain = cvec.transform(Xtrain)\n",
    "Xtest = cvec.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def text_prep(the_text):\n",
    "    the_text = the_text.lower()\n",
    "    the_text = tokenizer.tokenize(the_text)\n",
    "    for i in the_text:\n",
    "        new_text += re.sub(\"[^a-zA-A]\",\" \", i)\n",
    "    new_text = lemmatizer.lemmatize(new_text)\n",
    "    #the_text = [x.split() for x in the_text if not x in stopwords.words('english')]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A scrapped Function\n",
    "def cvec_it(df):\n",
    "    cvec_selftext = cvec.fit_transform(df['selftext'].fillna(' '))\n",
    "    text_df = pd.DataFrame(cvec_selftext.todense(),columns=cvec.vocabulary_)\n",
    "    cvec_title = cvec.fit_transform(df['title'].fillna(' '))\n",
    "    title_df = pd.DataFrame(cvec_title.todense(),columns=cvec.vocabulary_)\n",
    "    print('concatinating...')\n",
    "    return text_df.add(title_df, fill_value=0, axis=0).astype('int64')\n",
    "cvec_lesbians = cvec_it(df_lesbians)\n",
    "#cvec_femcels = cvec_it(df_femcels)\n",
    "cvec_incels = cvec_it(df_incels)\n",
    "#cvec_gaybros = cvec_it(df_gaybros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the X and y via accedental cvecing before the train/test split\n",
    "# X = cvec.fit_transform(les_or_inc['all_text'])\n",
    "# y = les_or_inc['is_lesbians']\n",
    "\n",
    "#Train Test Split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "#                                                    random_state=76)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train = cvec.transform(X_train)\n",
    "X_test = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_seduction = scrape_reddit('seduction')\n",
    "df_puascience = scrape_reddit('puascience')\n",
    "df_pickup = scrape_reddit('pickup')\n",
    "df_pua = scrape_reddit('pua')\n",
    "df_lgbt = scrape_reddit('lgbt')\n",
    "df_ainbow = scrape_reddit('ainbow')\n",
    "df_AskLGBT = scrape_reddit('AskLGBT')\n",
    "df_happentobegay = scrape_reddit('happentobegay')\n",
    "df_glbt = scrape_reddit('glbt')\n",
    "df_gay = scrape_reddit('gay')\n",
    "df_lgb = scrape_reddit('lgb')\n",
    "df_gayrights = scrape_reddit('gayrights')\n",
    "df_LGBTVent = scrape_reddit('LGBTVent')\n",
    "df_lgbtsex = scrape_reddit('lgbtsex')\n",
    "df_queer = scrape_reddit('queer')\n",
    "df_asexual = scrape_reddit('asexual')\n",
    "df_asexuality = scrape_reddit('asexuality')\n",
    "df_bisexual = scrape_reddit('bisexual')\n",
    "df_pansexual = scrape_reddit('pansexual')\n",
    "df_gaygeek = scrape_reddit('gaygeek')\n",
    "df_gaymers = scrape_reddit('gaymere')\n",
    "df_q4q = scrape_reddit('q4q')\n",
    "df_GaymersGoneMild = scrape_reddit('GaymersGoneMild')\n",
    "df_lgbtcirclejerk = scrape_reddit('lgbtcirclejerk')\n",
    "df_QPOC = scrape_reddit('QPOC')\n",
    "df_TransHack = scrape_reddit('TransHack')\n",
    "df_QueerFashionAdvice = scrape_reddit('QueerFashionAdvice')\n",
    "df_QueerTransmen = scrape_reddit('QueerTransmen')\n",
    "df_asktransgender = scrape_reddit('asktransgender')\n",
    "df_androgynoushotties = scrape_reddit('androgynoushotties')\n",
    "df_GirlGamers = scrape_reddit('GirlGamers')\n",
    "df_men = scrape_reddit('men')\n",
    "df_OneY = scrape_reddit('OneY')\n",
    "df_TwoXChromosomes = scrape_reddit('TwoXChromosomes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_seduction.to_csv('seduction_10_30', index = False)\n",
    "#df_puascience.to_csv('puascience_10_30', index = False)\n",
    "df_pickup.to_csv('pickup_10_30', index = False)\n",
    "#df_pua.to_csv('pua_10_30', index = False)\n",
    "df_lgbt.to_csv('lgbt_10_30', index = False)\n",
    "df_ainbow.to_csv('ainbow_10_30', index = False)\n",
    "df_AskLGBT.to_csv('AskLGBT_10_30', index = False)\n",
    "df_happentobegay.to_csv('happentobegay_10_30', index = False)\n",
    "df_glbt.to_csv('glbt_10_30', index = False)\n",
    "df_gay.to_csv('gay_10_30', index = False)\n",
    "#df_lgb.to_csv('lgb_10_30', index = False)\n",
    "df_gayrights.to_csv('gayrights_10_30', index = False)\n",
    "#df_LGBTVent.to_csv('LGBTVent_10_30', index = False)\n",
    "df_lgbtsex.to_csv('lgbtsex_10_30', index = False)\n",
    "df_queer.to_csv('queer_10_30', index = False)\n",
    "df_asexual.to_csv('asexual_10_30', index = False)\n",
    "df_asexuality.to_csv('asexuality_10_30', index = False)\n",
    "df_bisexual.to_csv('bisexual_10_30', index = False)\n",
    "df_pansexual.to_csv('pansexual_10_30', index = False)\n",
    "df_gaygeek.to_csv('gaygeek_10_30', index = False)\n",
    "#df_gaymers.to_csv('gaymere_10_30', index = False)\n",
    "df_q4q.to_csv('q4q_10_30', index = False)\n",
    "df_GaymersGoneMild.to_csv('GaymersGoneMild_10_30', index = False)\n",
    "df_lgbtcirclejerk.to_csv('lgbtcirclejerk_10_30', index = False)\n",
    "#df_QPOC.to_csv('QPOC_10_30', index = False)\n",
    "#df_TransHack.to_csv('TransHack_10_30', index = False)\n",
    "#df_QueerFashionAdvice.to_csv('QueerFashionAdvice_10_30', index = False)\n",
    "df_QueerTransmen.to_csv('QueerTransmen_10_30', index = False)\n",
    "df_asktransgender.to_csv('asktransgender_10_30', index = False)\n",
    "df_androgynoushotties.to_csv('androgynoushotties_10_30', index = False)\n",
    "df_GirlGamers.to_csv('GirlGamers_10_30', index = False)\n",
    "df_men.to_csv('men_10_30', index = False)\n",
    "df_OneY.to_csv('OneY_10_30', index = False)\n",
    "#df_TwoXChromosome.to_csv('TwoXChromosomes_10_30', index = False)\n",
    "#df_lesbians.to_csv('actuallesbians_9_9_400', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
