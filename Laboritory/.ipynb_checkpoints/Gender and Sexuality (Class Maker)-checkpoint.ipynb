{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "# Executive Summary\n",
    "How many times have you opened up a browser for a random subreddit only to find that it wasn't the random subreddit you were looking for?  We've all been there.  Furthermore, what about when you wonder \"golly, just how similar are different subreddits that are focused one concept but from entirely different points of view?\"  Well, we hear you.  We've scrapped data from two active subreddits which focus around sexuality and using them build a model that's able to detect if it's one subreddit or the other with over an 80% certainty.  Furthermore, if future exploritory data analysis, we hope to one day be able to talk about the defining features of each subculter that's being represented by these subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broken subreddits:\n",
    "    - puascience\n",
    "    - pua\n",
    "    - lgb\n",
    "    - LGBTVent\n",
    "    - QPOC\n",
    "    - TransHack\n",
    "    - QueerFashionAdvice\n",
    "    - TwoXChromosome\n",
    "    - lesbians\n",
    "    \n",
    "Redo: gaymers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions & attributes:\n",
    "    cvec\n",
    "     only text\n",
    "     only title\n",
    "     both\n",
    "    hvec\n",
    "    TF-IDF\n",
    "    # Memes & Images\n",
    "    most used words\n",
    "    Jargon\n",
    "    n-grams\n",
    "    Sentiment\n",
    "    .save\n",
    "        #Can we auto-create a data file if it doesn't already exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "pull_date = datetime.fromtimestamp(time()).strftime('%m_%d_%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([lemmer.lemmatize(word) for word in tokens if len(word) > 1 and not word in stop_words])\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                       min_df = 2,\n",
    "                       preprocessor = preprocess,\n",
    "                       stop_words = 'english') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Draft Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Class(object):\n",
    "    \"Discription of the class\"\n",
    "\n",
    "    def __init__(self, required_arg, stated_arg = 0.0): # Happens at initialization\n",
    "        \"\"\"Happens at initialization.  \n",
    "        This appears in the class description.\"\"\"\n",
    "        self.value_1 = required_arg                     # Required to be stated value\n",
    "        self.value_2 = stated_arg                       # Optional because stated argument\n",
    "        return\n",
    "\n",
    "    def __str__(self):\n",
    "        'Prints description of the class.'\n",
    "        return\n",
    "\n",
    "    \n",
    "    def regular_function(self, arg):\n",
    "        \"Appears in function description\"\n",
    "        if arg > self.balance:\n",
    "            raise RuntimeError('Arg > balance.')     # Error check\n",
    "        self.value_2 -= arg                             # Resaves self.value_2\n",
    "        return self.value_2                             # Returns/states value_2\n",
    "        \n",
    "    def __del__(self):\n",
    "        'Destructor; deletes an object.'\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class subreddit(object):\n",
    "    \n",
    "    def generate_from_api(self, the_subreddit, pages = 40):\n",
    "        all_posts = []\n",
    "        first_url = 'http://www.reddit.com/r/' + str(the_subreddit) + '.json'\n",
    "        url = first_url\n",
    "        list_of_df = []\n",
    "\n",
    "        for round in range(pages):\n",
    "            try:\n",
    "                res = requests.get(url, headers = {'User-agent':'Electronic Goddess'})\n",
    "                data = res.json()\n",
    "                list_of_posts = data['data']['children']\n",
    "                all_posts = all_posts + list_of_posts\n",
    "                after = data['data']['after']\n",
    "                url = first_url +'?after=' + after\n",
    "                print('Current After:' + after,'Round: '+ str(round + 1))\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                print('Limit likely hit.  Returning available posts.')\n",
    "                break\n",
    "\n",
    "        # Formats the parts we care about into a list of dictionaries that'll become the dataframe\n",
    "        for i in range(len(all_posts)):\n",
    "            index_dictionary = {\n",
    "                    'title' : all_posts[i]['data']['title'],\n",
    "                    'selftext': all_posts[i]['data']['selftext'],\n",
    "                    'subreddit' : all_posts[i]['data']['subreddit']\n",
    "                }\n",
    "            list_of_df.append(index_dictionary)\n",
    "        df = pd.DataFrame(list_of_df, columns = ['title','selftext','subreddit'])\n",
    "        return pd.DataFrame(list_of_df, columns = ['title','selftext','subreddit'])\n",
    "    \n",
    "    def __init__(self, reddit_title, from_archive = True, path = './Data/'):\n",
    "        if   from_archive == True:\n",
    "            self.df = pd.read_csv(path + str(reddit_title))\n",
    "        elif from_archive == False: \n",
    "            self.df = generate_from_api(reddit_title)\n",
    "            pull_date = datetime.fromtimestamp(time()).strftime('%m_%d_%Y')\n",
    "        else:\n",
    "            raise Error(\"WTF, load the df from somewhere.  It's literally everything\")\n",
    "        return\n",
    "    \n",
    "    def pull_from_csv(self, path):\n",
    "        df = pd.DataFrame.read_csv(path)\n",
    "        return pd.DataFrame.read_csv(path)\n",
    "    #def save(self):\n",
    "    #    self.df.to_read('./Data/) #path + redditname + pulldate\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Testing Labratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lezies = subreddit('AskLGBT_10_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lezies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ed3657a31e81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlezies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lezies' is not defined"
     ]
    }
   ],
   "source": [
    "lezies.df.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Human = Customer('Human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other\n"
     ]
    }
   ],
   "source": [
    "Human.deposit(2)\n",
    "print('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subreddit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6d8fb0021ada>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubreddit\u001b[0m\u001b[0;31m#.pull_from_csv()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'subreddit' is not defined"
     ]
    }
   ],
   "source": [
    "test = subreddit#.pull_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'subreddit' has no attribute 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7184ab9d30ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'subreddit' has no attribute 'df'"
     ]
    }
   ],
   "source": [
    "test.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pearl/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel/__main__.py:36: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "/Users/pearl/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel/__main__.py:37: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Reminder: please only post selfies/pictures of yourself in the Wednesday and Saturday mega threads</th>\n",
       "      <td>Lately there's been quite a few selfies/self p...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sunday Daily Chat Thread</th>\n",
       "      <td>Welcome to the daily chat thread! These are a ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>üòçüòç im gay</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Me as a girlfriend</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I can feel it in me bones</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I know these are advertised at best friends but all I could think about is how cute it would be to wear with a girlfriend</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If you‚Äôre from the west and just saw India decriminalize lgbt sex</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Girl: *does literally anything*</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kittens will also do the trick.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yup... I should have caught that.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lmaooo I made this</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is this you too?</th>\n",
       "      <td>Is it just me? Or do some of you out there als...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yassss</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Why tho</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Holy Mary Mother of God, Eva Green</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hmmmmm.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never stop running your hand through your hair</th>\n",
       "      <td>nothing knackers my banger on a barbie more th...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Holy shit you guys...</th>\n",
       "      <td>Apparently there's a TV show called All Girls ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I feel called out (minor OITNB s6 spoilers, maybe?)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>When the girl you like is sending you mixed messages</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Just a silly little thing I realized</th>\n",
       "      <td>I recently realized something. Growing up I al...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>There goes my HeroüôÉ</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wishing I had someone to watch the sunrise with.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"Adventure Time\": just Gal Pals liking each other a lot!</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ladies of the subreddit, I present...</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recorded before I was born, but Joan Jett is the hottest woman I've ever seen</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Look at these little potatoes üòªüíú</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I think Aubrey Plaza just made me gayer.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All I want is pizza and a cute girl dancing next to me tbh.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Love all you wholesome women</th>\n",
       "      <td>Ok, is anyone gonna talk about how cute and wh...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How do I figure out my feelings for girls are real...</th>\n",
       "      <td>How do I figure out my feelings for girls are ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>While you were closeted</th>\n",
       "      <td>while you were closeted, did you ever hate or ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I was 0/7 as a Child. Thanks Abstinence Only Education!</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Even if it's a kid's show, this is still my favourite lesbian couple.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Desiring a closeted religious bi girl: a heartache in vignettes</th>\n",
       "      <td>you breathe my name out like a prayer with the...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saw a commercial for this movie and i'm left thinking ... \"But is it gay?\"</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You Guys Are Awesome!</th>\n",
       "      <td>Since coming to terms with my sexuality I feel...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good Seattle places for queer women?</th>\n",
       "      <td>I've been a Seattleite all my life and I don't...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How do you feel about lesbians who sexually objectify women?</th>\n",
       "      <td>I was watching Dead Sara's music video for \"Mo...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How can I tell if someone is into me?</th>\n",
       "      <td>I'm back with another hypothetical! Add fast a...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sapphic pendants?</th>\n",
       "      <td>Fellow leslies! \\n\\nI‚Äôm a rather femme gal and...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my friend needs help</th>\n",
       "      <td>hello my friend is transitioning and she reall...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I really love her</th>\n",
       "      <td>So I already posted about this topic before bu...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Juggler</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Potential First Girlfriend</th>\n",
       "      <td>I started talking to this girl on Tinder a few...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conflicting information</th>\n",
       "      <td>I'm rather inexperienced with pretty much all ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFF or Girlfriend?</th>\n",
       "      <td>Hi all, I need some advice!\\n\\nSo my best frie...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uhm... I just fell in loveüôà</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Even Sappho fell in love with a straight girl</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strongly considering losing my virginity to a woman from Tinder, thoughts?</th>\n",
       "      <td>I‚Äôm a 25 year old girl who hasn‚Äôt had sex yet ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Any ideas on how to help her smile ...</th>\n",
       "      <td>Hey guys, I have posted on here before about m...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dating is feeling like a huge bummer</th>\n",
       "      <td>I'm just not finding anyone I connect with and...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Halo!</th>\n",
       "      <td>I know this may not be appropriate and if not ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asking Coworker to Lunch..</th>\n",
       "      <td>How do you as a coworker to lunch without it b...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Was trying to explain to my mom I wanted to wear a suit to a wedding</th>\n",
       "      <td>Me: well it‚Äôs in late October so I don‚Äôt want ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Looking for some advice</th>\n",
       "      <td>So I started chatting with this girl on OkCupi...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lesbian Youtubers</th>\n",
       "      <td>Hey all! I want to subscribe to more lesbian Y...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Had to share</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Halsey Plz</th>\n",
       "      <td>NaN</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fairly new to the board and wanted to say hi!</th>\n",
       "      <td>Hey fellow Raptors! I have been spending more ...</td>\n",
       "      <td>actuallesbians</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>993 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             selftext  \\\n",
       "title                                                                                                   \n",
       "Reminder: please only post selfies/pictures of ...  Lately there's been quite a few selfies/self p...   \n",
       "Sunday Daily Chat Thread                            Welcome to the daily chat thread! These are a ...   \n",
       "üòçüòç im gay                                                                                         NaN   \n",
       "Me as a girlfriend                                                                                NaN   \n",
       "I can feel it in me bones                                                                         NaN   \n",
       "I know these are advertised at best friends but...                                                NaN   \n",
       "If you‚Äôre from the west and just saw India decr...                                                NaN   \n",
       "Girl: *does literally anything*                                                                   NaN   \n",
       "Kittens will also do the trick.                                                                   NaN   \n",
       "Yup... I should have caught that.                                                                 NaN   \n",
       "Lmaooo I made this                                                                                NaN   \n",
       "Is this you too?                                    Is it just me? Or do some of you out there als...   \n",
       "yassss                                                                                            NaN   \n",
       "Why tho                                                                                           NaN   \n",
       "Holy Mary Mother of God, Eva Green                                                                NaN   \n",
       "Hmmmmm.                                                                                           NaN   \n",
       "never stop running your hand through your hair      nothing knackers my banger on a barbie more th...   \n",
       "Holy shit you guys...                               Apparently there's a TV show called All Girls ...   \n",
       "I feel called out (minor OITNB s6 spoilers, may...                                                NaN   \n",
       "When the girl you like is sending you mixed mes...                                                NaN   \n",
       "Just a silly little thing I realized                I recently realized something. Growing up I al...   \n",
       "There goes my HeroüôÉ                                                                               NaN   \n",
       "Wishing I had someone to watch the sunrise with.                                                  NaN   \n",
       "\"Adventure Time\": just Gal Pals liking each oth...                                                NaN   \n",
       "Ladies of the subreddit, I present...                                                             NaN   \n",
       "Recorded before I was born, but Joan Jett is th...                                                NaN   \n",
       "Look at these little potatoes üòªüíú                                                                  NaN   \n",
       "I think Aubrey Plaza just made me gayer.                                                          NaN   \n",
       "All I want is pizza and a cute girl dancing nex...                                                NaN   \n",
       "Love all you wholesome women                        Ok, is anyone gonna talk about how cute and wh...   \n",
       "...                                                                                               ...   \n",
       "How do I figure out my feelings for girls are r...  How do I figure out my feelings for girls are ...   \n",
       "While you were closeted                             while you were closeted, did you ever hate or ...   \n",
       "I was 0/7 as a Child. Thanks Abstinence Only Ed...                                                NaN   \n",
       "Even if it's a kid's show, this is still my fav...                                                NaN   \n",
       "Desiring a closeted religious bi girl: a hearta...  you breathe my name out like a prayer with the...   \n",
       "Saw a commercial for this movie and i'm left th...                                                NaN   \n",
       "You Guys Are Awesome!                               Since coming to terms with my sexuality I feel...   \n",
       "Good Seattle places for queer women?                I've been a Seattleite all my life and I don't...   \n",
       "How do you feel about lesbians who sexually obj...  I was watching Dead Sara's music video for \"Mo...   \n",
       "How can I tell if someone is into me?               I'm back with another hypothetical! Add fast a...   \n",
       "Sapphic pendants?                                   Fellow leslies! \\n\\nI‚Äôm a rather femme gal and...   \n",
       "my friend needs help                                hello my friend is transitioning and she reall...   \n",
       "I really love her                                   So I already posted about this topic before bu...   \n",
       "Juggler                                                                                           NaN   \n",
       "Potential First Girlfriend                          I started talking to this girl on Tinder a few...   \n",
       "Conflicting information                             I'm rather inexperienced with pretty much all ...   \n",
       "BFF or Girlfriend?                                  Hi all, I need some advice!\\n\\nSo my best frie...   \n",
       "Uhm... I just fell in loveüôà                                                                       NaN   \n",
       "Even Sappho fell in love with a straight girl                                                     NaN   \n",
       "Strongly considering losing my virginity to a w...  I‚Äôm a 25 year old girl who hasn‚Äôt had sex yet ...   \n",
       "Any ideas on how to help her smile ...              Hey guys, I have posted on here before about m...   \n",
       "Dating is feeling like a huge bummer                I'm just not finding anyone I connect with and...   \n",
       "Halo!                                               I know this may not be appropriate and if not ...   \n",
       "Asking Coworker to Lunch..                          How do you as a coworker to lunch without it b...   \n",
       "Was trying to explain to my mom I wanted to wea...  Me: well it‚Äôs in late October so I don‚Äôt want ...   \n",
       "Looking for some advice                             So I started chatting with this girl on OkCupi...   \n",
       "Lesbian Youtubers                                   Hey all! I want to subscribe to more lesbian Y...   \n",
       "Had to share                                                                                      NaN   \n",
       "Halsey Plz                                                                                        NaN   \n",
       "Fairly new to the board and wanted to say hi!       Hey fellow Raptors! I have been spending more ...   \n",
       "\n",
       "                                                         subreddit  \n",
       "title                                                               \n",
       "Reminder: please only post selfies/pictures of ...  actuallesbians  \n",
       "Sunday Daily Chat Thread                            actuallesbians  \n",
       "üòçüòç im gay                                           actuallesbians  \n",
       "Me as a girlfriend                                  actuallesbians  \n",
       "I can feel it in me bones                           actuallesbians  \n",
       "I know these are advertised at best friends but...  actuallesbians  \n",
       "If you‚Äôre from the west and just saw India decr...  actuallesbians  \n",
       "Girl: *does literally anything*                     actuallesbians  \n",
       "Kittens will also do the trick.                     actuallesbians  \n",
       "Yup... I should have caught that.                   actuallesbians  \n",
       "Lmaooo I made this                                  actuallesbians  \n",
       "Is this you too?                                    actuallesbians  \n",
       "yassss                                              actuallesbians  \n",
       "Why tho                                             actuallesbians  \n",
       "Holy Mary Mother of God, Eva Green                  actuallesbians  \n",
       "Hmmmmm.                                             actuallesbians  \n",
       "never stop running your hand through your hair      actuallesbians  \n",
       "Holy shit you guys...                               actuallesbians  \n",
       "I feel called out (minor OITNB s6 spoilers, may...  actuallesbians  \n",
       "When the girl you like is sending you mixed mes...  actuallesbians  \n",
       "Just a silly little thing I realized                actuallesbians  \n",
       "There goes my HeroüôÉ                                 actuallesbians  \n",
       "Wishing I had someone to watch the sunrise with.    actuallesbians  \n",
       "\"Adventure Time\": just Gal Pals liking each oth...  actuallesbians  \n",
       "Ladies of the subreddit, I present...               actuallesbians  \n",
       "Recorded before I was born, but Joan Jett is th...  actuallesbians  \n",
       "Look at these little potatoes üòªüíú                    actuallesbians  \n",
       "I think Aubrey Plaza just made me gayer.            actuallesbians  \n",
       "All I want is pizza and a cute girl dancing nex...  actuallesbians  \n",
       "Love all you wholesome women                        actuallesbians  \n",
       "...                                                            ...  \n",
       "How do I figure out my feelings for girls are r...  actuallesbians  \n",
       "While you were closeted                             actuallesbians  \n",
       "I was 0/7 as a Child. Thanks Abstinence Only Ed...  actuallesbians  \n",
       "Even if it's a kid's show, this is still my fav...  actuallesbians  \n",
       "Desiring a closeted religious bi girl: a hearta...  actuallesbians  \n",
       "Saw a commercial for this movie and i'm left th...  actuallesbians  \n",
       "You Guys Are Awesome!                               actuallesbians  \n",
       "Good Seattle places for queer women?                actuallesbians  \n",
       "How do you feel about lesbians who sexually obj...  actuallesbians  \n",
       "How can I tell if someone is into me?               actuallesbians  \n",
       "Sapphic pendants?                                   actuallesbians  \n",
       "my friend needs help                                actuallesbians  \n",
       "I really love her                                   actuallesbians  \n",
       "Juggler                                             actuallesbians  \n",
       "Potential First Girlfriend                          actuallesbians  \n",
       "Conflicting information                             actuallesbians  \n",
       "BFF or Girlfriend?                                  actuallesbians  \n",
       "Uhm... I just fell in loveüôà                         actuallesbians  \n",
       "Even Sappho fell in love with a straight girl       actuallesbians  \n",
       "Strongly considering losing my virginity to a w...  actuallesbians  \n",
       "Any ideas on how to help her smile ...              actuallesbians  \n",
       "Dating is feeling like a huge bummer                actuallesbians  \n",
       "Halo!                                               actuallesbians  \n",
       "Asking Coworker to Lunch..                          actuallesbians  \n",
       "Was trying to explain to my mom I wanted to wea...  actuallesbians  \n",
       "Looking for some advice                             actuallesbians  \n",
       "Lesbian Youtubers                                   actuallesbians  \n",
       "Had to share                                        actuallesbians  \n",
       "Halsey Plz                                          actuallesbians  \n",
       "Fairly new to the board and wanted to say hi!       actuallesbians  \n",
       "\n",
       "[993 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.pull_from_csv('../Laboritory/Data/actuallesbians_9_9_400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing EDA pre-Class Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_words(cveced_df):\n",
    "    return cveced_df.sum().sort_values()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvec_df(df):\n",
    "    # Single Forum into a Cvec for EDA\n",
    "    df.fillna('', inplace=True)\n",
    "\n",
    "    # Combining the title and selftext columns\n",
    "    df['all_text'] = df['title'] + ' ' + df['selftext']\n",
    "\n",
    "    # Resetting the Index\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Creating Cvec DataFrame of both forums\n",
    "    return pd.DataFrame(cvec.fit_transform(df['all_text']).todense(), \n",
    "                            columns=cvec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvec_dfs_together(first_df, second_df, target_title = \"is_first_df\"):    # Identifying the y Values\n",
    "    temp_1_df = first_df\n",
    "    temp_2_df = second_df\n",
    "    \n",
    "    temp_1_df[target_title] = 1\n",
    "    temp_2_df[target_title] = 0\n",
    "\n",
    "    # Concatination of the two subreddits\n",
    "    dfs = pd.concat([temp_1_df.drop('subreddit', axis=1),\n",
    "                            temp_2_df.drop('subreddit', axis=1)],\n",
    "                            sort=True)\n",
    "\n",
    "    # Filling Nulls\n",
    "    dfs.fillna('', inplace=True)\n",
    "\n",
    "    # Combining the title and selftext columns\n",
    "    dfs['all_text'] = dfs['title'] + ' ' + dfs['selftext']\n",
    "\n",
    "    # Resetting the Index\n",
    "    dfs.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Creating Cvec DataFrame of both forums\n",
    "    cvec_dfs = pd.DataFrame(cvec.fit_transform(dfs['all_text']).todense(), \n",
    "                            columns=cvec.get_feature_names())\n",
    "\n",
    "    # Inserting the target column\n",
    "    cvec_dfs[target_title] = dfs[target_title]\n",
    "    return cvec_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the correlations to the two data frames.\n",
    "# 1 = represents coming from 1st subreddit.\n",
    "# 0 = represents coming from 2nd subreddit.\n",
    "def correlation_of_words(cvec_dfs, target_title = \"is_first_df\")\n",
    "    return df_words.corr().sort_values([target_title])[target_title]\n",
    "# df_corrs.tail(20)[18::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(df_column, num_topics = 3, num_words = 5):\n",
    "    # cols are the words\n",
    "    # rows are the topics\n",
    "    topic_lists = []\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, \n",
    "                                    learning_method='online'\n",
    "                                    ) # Learning meathod stated for depreciation \n",
    "    lda.fit(cvec.fit_transform(df_column))\n",
    "    for ix, topic in enumerate(lda.components_):\n",
    "        topic_lists += [[cvec.get_feature_names()[i] for i \n",
    "                         in lda.components_[ix].argsort()[:-num_words - 1:-1]]]\n",
    "\n",
    "    return pd.DataFrame(topic_lists, columns=[ 'word_' + str(i) for i \n",
    "                                              in range(1, num_words+1)], \n",
    "                 index=range(1, num_topics + 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving for when moved to the class code \n",
    "# def save_subreddit(subreddit_df, pull_date, subreddit_title, path = \"./Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Struggle: I decided to change the code for LDA such that instead of printing out each individual topic it would instead generate a temporary dataframe.  Mostly this transition was made via lists.  As one might be able to see in the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Laboritory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspergirls\n",
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Current After:t3_aaf7hy Round: 1\n",
      "Current After:t3_aa48v7 Round: 2\n",
      "Current After:t3_a91raw Round: 3\n",
      "Current After:t3_a7qm2o Round: 4\n",
      "Current After:t3_a72bwp Round: 5\n",
      "Current After:t3_a5w11f Round: 6\n",
      "Current After:t3_a505ru Round: 7\n",
      "Current After:t3_a45vm4 Round: 8\n",
      "Current After:t3_a366yc Round: 9\n",
      "Current After:t3_a2cmfw Round: 10\n",
      "Current After:t3_a1rjxv Round: 11\n",
      "Current After:t3_a0wi93 Round: 12\n",
      "Current After:t3_9z7ygo Round: 13\n",
      "Current After:t3_9y5m62 Round: 14\n",
      "Current After:t3_9wyftf Round: 15\n",
      "Current After:t3_9vxetq Round: 16\n",
      "Current After:t3_9usmd9 Round: 17\n",
      "Current After:t3_9u8mek Round: 18\n",
      "Current After:t3_9szp3f Round: 19\n",
      "Current After:t3_9s811m Round: 20\n",
      "Current After:t3_9qt3zx Round: 21\n",
      "Current After:t3_9pqdty Round: 22\n",
      "Current After:t3_9oeuxo Round: 23\n",
      "Current After:t3_9ngl6n Round: 24\n",
      "Current After:t3_9minid Round: 25\n",
      "Current After:t3_9l6n2y Round: 26\n",
      "Current After:t3_9k3pjf Round: 27\n",
      "Current After:t3_9izu7x Round: 28\n",
      "Current After:t3_9hl037 Round: 29\n",
      "Current After:t3_9fvasl Round: 30\n",
      "Current After:t3_9f51ro Round: 31\n",
      "Current After:t3_9e4v5k Round: 32\n",
      "Current After:t3_9d7jfy Round: 33\n",
      "Current After:t3_9bxu5l Round: 34\n",
      "Current After:t3_9aac4h Round: 35\n",
      "Current After:t3_98fpxd Round: 36\n",
      "Limit likely hit.  Returning available posts.\n",
      "aspergers\n",
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Current After:t3_abne6a Round: 1\n",
      "Current After:t3_ab63iz Round: 2\n",
      "Current After:t3_abf0pg Round: 3\n",
      "Current After:t3_abbjh4 Round: 4\n",
      "Current After:t3_aay4ft Round: 5\n",
      "Current After:t3_aaq53i Round: 6\n",
      "Current After:t3_aalkwq Round: 7\n",
      "Current After:t3_aai4ii Round: 8\n",
      "Current After:t3_aa83rj Round: 9\n",
      "Current After:t3_aa2cg4 Round: 10\n",
      "Current After:t3_aa2ohm Round: 11\n",
      "Current After:t3_a9vjow Round: 12\n",
      "Current After:t3_a9jhcm Round: 13\n",
      "Current After:t3_a9gwwr Round: 14\n",
      "Current After:t3_a9cupq Round: 15\n",
      "Current After:t3_a94gzt Round: 16\n",
      "Current After:t3_a8w6gc Round: 17\n",
      "Current After:t3_a8qzoe Round: 18\n",
      "Current After:t3_a8m1wu Round: 19\n",
      "Current After:t3_a8ep6x Round: 20\n",
      "Current After:t3_a87efp Round: 21\n",
      "Current After:t3_a7yr09 Round: 22\n",
      "Current After:t3_a80tgr Round: 23\n",
      "Current After:t3_a7osgi Round: 24\n",
      "Current After:t3_a7k8pq Round: 25\n",
      "Current After:t3_a75onc Round: 26\n",
      "Current After:t3_a700xb Round: 27\n",
      "Current After:t3_a72t2i Round: 28\n",
      "Current After:t3_a6q0lk Round: 29\n",
      "Current After:t3_a6vfzh Round: 30\n",
      "Current After:t3_a6e8b2 Round: 31\n",
      "Current After:t3_a6g3ma Round: 32\n",
      "Current After:t3_a6eaz8 Round: 33\n",
      "Current After:t3_a60anz Round: 34\n",
      "Current After:t3_a60ibb Round: 35\n",
      "Current After:t3_a5rxfh Round: 36\n",
      "Current After:t3_a5p2jh Round: 37\n",
      "Current After:t3_a5p871 Round: 38\n",
      "Current After:t3_a5h9qs Round: 39\n",
      "Limit likely hit.  Returning available posts.\n",
      "AspiePartners\n",
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Current After:t3_9zy8ju Round: 1\n",
      "Current After:t3_9n1pg0 Round: 2\n",
      "Current After:t3_9c1xik Round: 3\n",
      "Current After:t3_8yctca Round: 4\n",
      "Current After:t3_8p6bjj Round: 5\n",
      "Current After:t3_848zfm Round: 6\n",
      "Current After:t3_7kp4z1 Round: 7\n",
      "Current After:t3_6ydxvh Round: 8\n",
      "Current After:t3_6lzmjy Round: 9\n",
      "Current After:t3_5swbfk Round: 10\n",
      "Current After:t3_4m0vol Round: 11\n",
      "Current After:t3_48q7nq Round: 12\n",
      "Current After:t3_3ggh39 Round: 13\n",
      "Current After:t3_2yujzy Round: 14\n",
      "Limit likely hit.  Returning available posts.\n"
     ]
    }
   ],
   "source": [
    "df_aspergirls = scrape_reddit('aspergirls')\n",
    "df_aspergers = scrape_reddit('aspergers')\n",
    "df_AspiePartners = scrape_reddit('AspiePartners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA(df_aspergirls['title'] + ' ' + df_aspergirls['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welcome to r/Aspergirls! Start here for genera...</td>\n",
       "      <td>Welcome to r/Aspergirls! We are a community fo...</td>\n",
       "      <td>aspergirls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Year in Review: How did your 2018 go? What are...</td>\n",
       "      <td>I‚Äôm going to start this thread off with some 2...</td>\n",
       "      <td>aspergirls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Executive functioning help</td>\n",
       "      <td>About me: I'm 19, and in my second year of uni...</td>\n",
       "      <td>aspergirls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just realized my (step)great-grandmother (92...</td>\n",
       "      <td>Today, my uncle complained how my great-grandm...</td>\n",
       "      <td>aspergirls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I tell my boyfriend that I want him to ...</td>\n",
       "      <td>I have a problem with just including myself in...</td>\n",
       "      <td>aspergirls</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Welcome to r/Aspergirls! Start here for genera...   \n",
       "1  Year in Review: How did your 2018 go? What are...   \n",
       "2                         Executive functioning help   \n",
       "3  I just realized my (step)great-grandmother (92...   \n",
       "4  How do I tell my boyfriend that I want him to ...   \n",
       "\n",
       "                                            selftext   subreddit  \n",
       "0  Welcome to r/Aspergirls! We are a community fo...  aspergirls  \n",
       "1  I‚Äôm going to start this thread off with some 2...  aspergirls  \n",
       "2  About me: I'm 19, and in my second year of uni...  aspergirls  \n",
       "3  Today, my uncle complained how my great-grandm...  aspergirls  \n",
       "4  I have a problem with just including myself in...  aspergirls  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aspergirls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "like      1221\n",
       "feel       758\n",
       "know       722\n",
       "people     688\n",
       "thing      648\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words(cvec_aspergirls).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6108179419525066"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1221/758"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy Goodness gracious.  Aspergirls uses the word \"like\" 1.6 times more than the next most used word.  (Based on data pulled on January 1, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AskReddit\n",
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Current After:t3_ablj0e Round: 1\n",
      "Current After:t3_abcpp6 Round: 2\n",
      "Current After:t3_abn7et Round: 3\n",
      "Current After:t3_abn9ho Round: 4\n",
      "Current After:t3_abn6vb Round: 5\n",
      "Current After:t3_abi4lf Round: 6\n",
      "Current After:t3_abn63x Round: 7\n",
      "Current After:t3_abmqld Round: 8\n",
      "Current After:t3_abn8ng Round: 9\n",
      "Current After:t3_abkwru Round: 10\n",
      "Current After:t3_ablja2 Round: 11\n",
      "Current After:t3_abngjs Round: 12\n",
      "Current After:t3_abmhix Round: 13\n",
      "Current After:t3_abn8tj Round: 14\n",
      "Current After:t3_abn6ws Round: 15\n",
      "Current After:t3_abn459 Round: 16\n",
      "Current After:t3_abm482 Round: 17\n",
      "Current After:t3_ablztt Round: 18\n",
      "Current After:t3_abmqiz Round: 19\n",
      "Current After:t3_abmnw6 Round: 20\n",
      "Current After:t3_abkdj8 Round: 21\n",
      "Current After:t3_abmeoq Round: 22\n",
      "Current After:t3_abnk7o Round: 23\n",
      "Current After:t3_abnhz7 Round: 24\n",
      "Current After:t3_abjnt5 Round: 25\n",
      "Current After:t3_abm6cs Round: 26\n",
      "Current After:t3_abnbuc Round: 27\n",
      "Current After:t3_abl5wx Round: 28\n",
      "Current After:t3_abjvkb Round: 29\n",
      "Current After:t3_abn5ej Round: 30\n",
      "Current After:t3_abkyr2 Round: 31\n",
      "Current After:t3_abkw9k Round: 32\n",
      "Current After:t3_abkucr Round: 33\n",
      "Current After:t3_abmwns Round: 34\n",
      "Current After:t3_abk100 Round: 35\n",
      "Current After:t3_abjyig Round: 36\n",
      "Current After:t3_abmptf Round: 37\n",
      "Current After:t3_abkhxc Round: 38\n",
      "Current After:t3_abmk0p Round: 39\n",
      "Limit likely hit.  Returning available posts.\n"
     ]
    }
   ],
   "source": [
    "Ask = scrape_reddit('AskReddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By compairing a Count Vectorized subreddit's posts to the AskReddit subreddit, we can start to get a baseline sense of the difference between the reddit subculture and the rest of reddit.  In this way, we can compair the group to something resembling a control group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most correlated to PUA subreddit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pickup     0.239630\n",
       "know       0.204987\n",
       "like       0.204593\n",
       "girl       0.193293\n",
       "got        0.192312\n",
       "guy        0.184053\n",
       "good       0.182273\n",
       "game       0.177244\n",
       "infield    0.176056\n",
       "time       0.172294\n",
       "talk       0.171338\n",
       "really     0.170853\n",
       "help       0.165919\n",
       "program    0.164854\n",
       "pick       0.162764\n",
       "want       0.162612\n",
       "let        0.161629\n",
       "request    0.158465\n",
       "share      0.156359\n",
       "Name: is_PUA, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying the y Values\n",
    "PUAs['is_PUA'] = 1\n",
    "Ask['is_PUA'] = 0\n",
    "\n",
    "# Concatination of the two subreddits\n",
    "PUA_or_not = pd.concat([PUAs.drop('subreddit', axis=1),\n",
    "                        Ask.drop('subreddit', axis=1)],\n",
    "                        sort=True)\n",
    "\n",
    "# Filling Nulls\n",
    "PUA_or_not.fillna('', inplace=True)\n",
    "\n",
    "# Combining the title and selftext columns\n",
    "PUA_or_not['all_text'] = PUA_or_not['title'] + ' ' + PUA_or_not['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "PUA_or_not.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Creating Cvec DataFrame of both forums\n",
    "df_words = pd.DataFrame(cvec.fit_transform(PUA_or_not['all_text']).todense(), \n",
    "                        columns=cvec.get_feature_names())\n",
    "\n",
    "# Inserting the target column\n",
    "df_words['is_PUA'] = PUA_or_not['is_PUA']\n",
    "# Listing the correlations to the two data frames.\n",
    "# 1 = represents coming from lesbians subreddit.\n",
    "# 0 = represents coming from incels subreddit.\n",
    "df_corrs = df_words.corr().sort_values(['is_PUA'])['is_PUA']\n",
    "print(\"Most correlated to PUA subreddit\")\n",
    "df_corrs.tail(20)[18::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ruined       -0.044586\n",
       "coolest      -0.044586\n",
       "eve          -0.047051\n",
       "craziest     -0.047051\n",
       "hobby        -0.047297\n",
       "tv           -0.047305\n",
       "dumbest      -0.049862\n",
       "worst        -0.050937\n",
       "funniest     -0.063120\n",
       "scariest     -0.063120\n",
       "theme        -0.063120\n",
       "favorite     -0.063866\n",
       "song         -0.067346\n",
       "favourite    -0.070606\n",
       "gamers       -0.074072\n",
       "weirdest     -0.074072\n",
       "resolution   -0.102943\n",
       "redditors    -0.109781\n",
       "reddit       -0.154910\n",
       "Name: is_PUA, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corrs.head(20)[18::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUAs.drop('level_0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating Cvec DataFrame of both forums\n",
    "df_words = pd.DataFrame(cvec.fit_transform(les_or_inc['all_text']).todense(), \n",
    "                        columns=cvec.get_feature_names())\n",
    "\n",
    "# Inserting the target column\n",
    "df_words['is_lesbians'] = les_or_inc['is_lesbians']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_aspergirls = cvec_df(df_aspergirls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absorbed</th>\n",
       "      <th>abstract</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtu</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 3935 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aba  aback  abandoned  ability  able  abroad  absolute  absolutely  \\\n",
       "0    0      0          0        0     0       0         0           0   \n",
       "1    0      0          0        0     0       0         0           0   \n",
       "2    0      0          0        0     0       0         0           0   \n",
       "3    0      0          0        0     1       0         0           0   \n",
       "4    0      0          0        0     0       0         0           0   \n",
       "\n",
       "   absorbed  abstract  ...    young  younger  youngest  youth  youtu  youtube  \\\n",
       "0         0         0  ...        0        0         0      0      0        0   \n",
       "1         0         0  ...        0        0         0      0      0        0   \n",
       "2         0         0  ...        0        0         0      0      0        0   \n",
       "3         0         0  ...        0        0         0      0      0        0   \n",
       "4         0         0  ...        0        0         0      0      0        0   \n",
       "\n",
       "   yr  zero  zone  zoned  \n",
       "0   0     0     0      0  \n",
       "1   0     0     0      0  \n",
       "2   0     0     0      0  \n",
       "3   0     0     0      0  \n",
       "4   0     0     0      0  \n",
       "\n",
       "[5 rows x 3935 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec_aspergirls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Forum into a Cvec for EDA\n",
    "PUAs.fillna('', inplace=True)\n",
    "\n",
    "# Combining the title and selftext columns\n",
    "PUAs['all_text'] = PUAs['title'] + ' ' + PUAs['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "#PUAs.reset_index(inplace=True)\n",
    "\n",
    "# Creating Cvec DataFrame of both forums\n",
    "PUA_words = pd.DataFrame(cvec.fit_transform(PUAs['all_text']).todense(), \n",
    "                        columns=cvec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUA_corrs = PUA_words.corr().sort_values(['is_lesbians'])['is_lesbians']\n",
    "print(\"Most correlated to Lesbians subreddit\")\n",
    "PUA_corrs.tail(20)[18::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "girl              675\n",
       "like              454\n",
       "guy               362\n",
       "woman             320\n",
       "game              287\n",
       "time              284\n",
       "want              269\n",
       "know              259\n",
       "rsd               236\n",
       "really            210\n",
       "pickup            210\n",
       "make              209\n",
       "friend            208\n",
       "good              208\n",
       "say               198\n",
       "day               193\n",
       "people            191\n",
       "think             186\n",
       "thing             185\n",
       "http              182\n",
       "going             182\n",
       "approach          169\n",
       "feel              166\n",
       "lot               156\n",
       "way               151\n",
       "year              148\n",
       "got               147\n",
       "life              146\n",
       "video             146\n",
       "start             142\n",
       "                 ... \n",
       "interacted          1\n",
       "interacting         1\n",
       "talent              1\n",
       "intuitively         1\n",
       "intuitive           1\n",
       "intuition           1\n",
       "introspection       1\n",
       "introductory        1\n",
       "introducing         1\n",
       "tallish             1\n",
       "intrigued           1\n",
       "intoxicating        1\n",
       "intonation          1\n",
       "intimidation        1\n",
       "intimedated         1\n",
       "intial              1\n",
       "intetesting         1\n",
       "interviwes          1\n",
       "tally               1\n",
       "intervention        1\n",
       "interspersed        1\n",
       "interrupting        1\n",
       "interpretation      1\n",
       "tangent             1\n",
       "internalizing       1\n",
       "tangible            1\n",
       "tanned              1\n",
       "interior            1\n",
       "tapping             1\n",
       "zzs                 1\n",
       "Length: 6832, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting most common words\n",
    "PUA_words.sum().sort_values()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaaww</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abc</th>\n",
       "      <th>abiding</th>\n",
       "      <th>ability</th>\n",
       "      <th>abit</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zota</th>\n",
       "      <th>zpbguvzc</th>\n",
       "      <th>zpzgvvlxryywluaw</th>\n",
       "      <th>ztrkmpgf</th>\n",
       "      <th>ztufd</th>\n",
       "      <th>zvjzyijeig</th>\n",
       "      <th>zy</th>\n",
       "      <th>zyzz</th>\n",
       "      <th>zzs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 6832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaaaand  aaaww  ab  aback  abbreviated  abc  abiding  ability  abit  \\\n",
       "0   0        0      0   0      0            0    0        0        0     0   \n",
       "1   0        0      0   0      0            0    0        0        0     0   \n",
       "2   0        0      0   0      0            0    0        0        0     0   \n",
       "3   0        0      0   0      0            0    0        0        0     0   \n",
       "4   0        0      0   0      0            0    0        0        0     0   \n",
       "\n",
       "  ...   zone  zota  zpbguvzc  zpzgvvlxryywluaw  ztrkmpgf  ztufd  zvjzyijeig  \\\n",
       "0 ...      0     0         0                 0         0      0           0   \n",
       "1 ...      0     0         0                 0         0      0           0   \n",
       "2 ...      0     0         0                 0         0      0           0   \n",
       "3 ...      0     0         0                 0         0      0           0   \n",
       "4 ...      0     0         0                 0         0      0           0   \n",
       "\n",
       "   zy  zyzz  zzs  \n",
       "0   0     0    0  \n",
       "1   0     0    0  \n",
       "2   0     0    0  \n",
       "3   0     0    0  \n",
       "4   0     0    0  \n",
       "\n",
       "[5 rows x 6832 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUA_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Moderation Team &amp;amp; other news!</td>\n",
       "      <td>Hello!\\n\\nThe r/Pickup subreddit now has a new...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to approach any girl! Indirect vs Direct! ...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[FR] First pull from daygame; feminists are no...</td>\n",
       "      <td>\\n\\nEnded up pulling from daygame yesterday f...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Getting back into the pick up game, but now wi...</td>\n",
       "      <td>I have been thinking about getting back into t...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please tell me I did not just miss, the best o...</td>\n",
       "      <td>You know the feeling when you think you had an...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Red Pill\" Theory</td>\n",
       "      <td>Thoughts on \"Red Pill\" theory for women? Any g...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Flirting 101 (Never Run Out of Things to Say)</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Story of how Julien went from Intern to Assist...</td>\n",
       "      <td>It has been 2 months since Julien had started ...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Here‚Äôs my success with Christmas pickup lines....</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you know these 5 signs you will know if she...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I used to be shy and this was my birthday</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RSD Max video where he talks about 'assuming t...</td>\n",
       "      <td>I remember he gave an example when if you're b...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Does Looks Matter To Girls</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How to Flip the Script - Get Her Chasing YOU</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How to Flip the Script - Get Her Chasing YOU</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Julien- Shaves Head And Gives Inspiring Semina...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How Get A Girl Through Instagram and Snapchat</td>\n",
       "      <td>Every man who is interested in attracting wome...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Dan Picking Up Not So Single MILFS</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Should I be going to nightclubs if I absolutel...</td>\n",
       "      <td>I am a newbie to game, and I fucking hate nigh...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I met everyone in a party!</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How To Get A Girl Through Snapchat and Instagr...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>How To Get More Dates (By Cultivating Positive...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How To Get A Girl To Come To Your House! With ...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Caught a gold digger...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>How To Get A Girl To Come To Your House</td>\n",
       "      <td>After hanging out with a girl for a few hours ...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[FR] A little field report</td>\n",
       "      <td>The first interaction of the night was 2 legit...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>My Personal Tips For Success</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How to start with Pickup ..</td>\n",
       "      <td>Hey guys.\\n\\nWARNING - YOUR EYES CAN BLEED FRO...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JMULV just got exposed</td>\n",
       "      <td>JMULV who claims to have the most recorded inf...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Five Types Of Millennium Girls To Avoid</td>\n",
       "      <td>Millennium girls are different from other girl...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Conversation Threading: Ideas on how to avoid ...</td>\n",
       "      <td>Since the inception of [/r/pickup](/r/pickup) ...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>[FR] APPROACH 25: NOT THE SAD DREAD</td>\n",
       "      <td>Approaches 12-24 were all sorts of weird awkwa...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>Tome of Knowledge 1: Using Some of this Shit</td>\n",
       "      <td>*This was written a while ago.. on DLHQ, befor...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>[Audio FR] I've been told I go into interview ...</td>\n",
       "      <td>Here is a recording of my sarge. I'm pretty ne...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>Recent Fuckups: A Compilation</td>\n",
       "      <td>This is purely for my own benefit, so that I m...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>XMP CONSPIRACY: SOMEONE IS STARTING TO SEE WHA...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>XMP CONSPIRACY: SEDUCES MEN INTO INTROSPECTION...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>[FR]Flustered turned into Fun</td>\n",
       "      <td>**TL:DR at the end**\\n\\n**Preface**\\n\\nSo earl...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>[FR] Breaking down the physical wall and rubbi...</td>\n",
       "      <td>**Preamble:**\\n\\nfriday night was a bust.  i h...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>DANTERING TALKS ABOUT TRANSACTIONAL ANALYSIS, ...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>IRONIX GETTIN SWOLE // PAF PARTNER PRESSES</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>SAN FRANCISCO MANVENTURE POW WOW...deets in co...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>BODYTALK: POSTURE AND SEXUAL FRAME, SHOOTIN TH...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>Want to ask Richard Bandler something? This mi...</td>\n",
       "      <td>I'm attending an introductory course on NLP by...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>[FR] Continued follow up with HB6</td>\n",
       "      <td>This is a 3rd part of this FR series. See prev...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>Sensory Acuity Flash Cards [NLP Practice]</td>\n",
       "      <td>I've compiled a set of 335 flash cards to broa...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>[FR] LEGENDARY'S EPIC: PIZZACLOSE, KCLOSE, @_@</td>\n",
       "      <td>PRELUDE:\\n\\nI finished my school for the day. ...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>[FR] - Meetup, Day 2 ish.</td>\n",
       "      <td>Ran into HBEngineer last week. I had met her t...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>A note about goals</td>\n",
       "      <td>So I decided to take a moment to point out a c...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>Questions Regarding Our Ethics</td>\n",
       "      <td>Hey boys, got some quick questions and deep sh...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>PAF Is The Shit</td>\n",
       "      <td>Sup playas, just wanted to chime in and bump a...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>On using the word \"I\" (ctrl-f for \"power dynam...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>Didn't sleep last night? Early in the morning?...</td>\n",
       "      <td>I really feel like I'm starting to flood the s...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>Public transit approaches, easiest daygame you...</td>\n",
       "      <td>**Where do you daygame if you're not in colleg...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>On being a giant puss</td>\n",
       "      <td>Yuck... I have spent the past two days wanderi...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>DANTERING SAYS: KEEP HER FOCUSED ON HER FEELS,...</td>\n",
       "      <td></td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>[FR] M1n1true battles through AA and puts some...</td>\n",
       "      <td>Excited to be posting my first FR, gents, but ...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>RE:Tok2:  I'm OK You're OK Book Report by Vita...</td>\n",
       "      <td>**I'm OK, You're OK** by Thomas Harris is prob...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>My Friday FR Report [Talk to everyone]</td>\n",
       "      <td>Hey /r/pickup, My name is Hypnotonic and here ...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Took 2 hours to go from meeting a stranger to ...</td>\n",
       "      <td>Yo. So there's this girl that I met at the beg...</td>\n",
       "      <td>pickup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>955 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0                New Moderation Team &amp; other news!   \n",
       "1    How to approach any girl! Indirect vs Direct! ...   \n",
       "2    [FR] First pull from daygame; feminists are no...   \n",
       "3    Getting back into the pick up game, but now wi...   \n",
       "4    Please tell me I did not just miss, the best o...   \n",
       "5                                    \"Red Pill\" Theory   \n",
       "6        Flirting 101 (Never Run Out of Things to Say)   \n",
       "7    Story of how Julien went from Intern to Assist...   \n",
       "8    Here‚Äôs my success with Christmas pickup lines....   \n",
       "9    If you know these 5 signs you will know if she...   \n",
       "10           I used to be shy and this was my birthday   \n",
       "11   RSD Max video where he talks about 'assuming t...   \n",
       "12                          Does Looks Matter To Girls   \n",
       "13        How to Flip the Script - Get Her Chasing YOU   \n",
       "14        How to Flip the Script - Get Her Chasing YOU   \n",
       "15   Julien- Shaves Head And Gives Inspiring Semina...   \n",
       "16       How Get A Girl Through Instagram and Snapchat   \n",
       "17                  Dan Picking Up Not So Single MILFS   \n",
       "18   Should I be going to nightclubs if I absolutel...   \n",
       "19                          I met everyone in a party!   \n",
       "20   How To Get A Girl Through Snapchat and Instagr...   \n",
       "21   How To Get More Dates (By Cultivating Positive...   \n",
       "22   How To Get A Girl To Come To Your House! With ...   \n",
       "23                             Caught a gold digger...   \n",
       "24             How To Get A Girl To Come To Your House   \n",
       "25                          [FR] A little field report   \n",
       "26                        My Personal Tips For Success   \n",
       "27                         How to start with Pickup ..   \n",
       "28                              JMULV just got exposed   \n",
       "29             Five Types Of Millennium Girls To Avoid   \n",
       "..                                                 ...   \n",
       "925  Conversation Threading: Ideas on how to avoid ...   \n",
       "926                [FR] APPROACH 25: NOT THE SAD DREAD   \n",
       "927       Tome of Knowledge 1: Using Some of this Shit   \n",
       "928  [Audio FR] I've been told I go into interview ...   \n",
       "929                      Recent Fuckups: A Compilation   \n",
       "930  XMP CONSPIRACY: SOMEONE IS STARTING TO SEE WHA...   \n",
       "931  XMP CONSPIRACY: SEDUCES MEN INTO INTROSPECTION...   \n",
       "932                      [FR]Flustered turned into Fun   \n",
       "933  [FR] Breaking down the physical wall and rubbi...   \n",
       "934  DANTERING TALKS ABOUT TRANSACTIONAL ANALYSIS, ...   \n",
       "935         IRONIX GETTIN SWOLE // PAF PARTNER PRESSES   \n",
       "936  SAN FRANCISCO MANVENTURE POW WOW...deets in co...   \n",
       "937  BODYTALK: POSTURE AND SEXUAL FRAME, SHOOTIN TH...   \n",
       "938  Want to ask Richard Bandler something? This mi...   \n",
       "939                  [FR] Continued follow up with HB6   \n",
       "940          Sensory Acuity Flash Cards [NLP Practice]   \n",
       "941     [FR] LEGENDARY'S EPIC: PIZZACLOSE, KCLOSE, @_@   \n",
       "942                          [FR] - Meetup, Day 2 ish.   \n",
       "943                                 A note about goals   \n",
       "944                     Questions Regarding Our Ethics   \n",
       "945                                    PAF Is The Shit   \n",
       "946  On using the word \"I\" (ctrl-f for \"power dynam...   \n",
       "947  Didn't sleep last night? Early in the morning?...   \n",
       "948  Public transit approaches, easiest daygame you...   \n",
       "949                              On being a giant puss   \n",
       "950  DANTERING SAYS: KEEP HER FOCUSED ON HER FEELS,...   \n",
       "951  [FR] M1n1true battles through AA and puts some...   \n",
       "952  RE:Tok2:  I'm OK You're OK Book Report by Vita...   \n",
       "953             My Friday FR Report [Talk to everyone]   \n",
       "954  Took 2 hours to go from meeting a stranger to ...   \n",
       "\n",
       "                                              selftext subreddit  \n",
       "0    Hello!\\n\\nThe r/Pickup subreddit now has a new...    pickup  \n",
       "1                                                         pickup  \n",
       "2     \\n\\nEnded up pulling from daygame yesterday f...    pickup  \n",
       "3    I have been thinking about getting back into t...    pickup  \n",
       "4    You know the feeling when you think you had an...    pickup  \n",
       "5    Thoughts on \"Red Pill\" theory for women? Any g...    pickup  \n",
       "6                                                         pickup  \n",
       "7    It has been 2 months since Julien had started ...    pickup  \n",
       "8                                                         pickup  \n",
       "9                                                         pickup  \n",
       "10                                                        pickup  \n",
       "11   I remember he gave an example when if you're b...    pickup  \n",
       "12                                                        pickup  \n",
       "13                                                        pickup  \n",
       "14                                                        pickup  \n",
       "15                                                        pickup  \n",
       "16   Every man who is interested in attracting wome...    pickup  \n",
       "17                                                        pickup  \n",
       "18   I am a newbie to game, and I fucking hate nigh...    pickup  \n",
       "19                                                        pickup  \n",
       "20                                                        pickup  \n",
       "21                                                        pickup  \n",
       "22                                                        pickup  \n",
       "23                                                        pickup  \n",
       "24   After hanging out with a girl for a few hours ...    pickup  \n",
       "25   The first interaction of the night was 2 legit...    pickup  \n",
       "26                                                        pickup  \n",
       "27   Hey guys.\\n\\nWARNING - YOUR EYES CAN BLEED FRO...    pickup  \n",
       "28   JMULV who claims to have the most recorded inf...    pickup  \n",
       "29   Millennium girls are different from other girl...    pickup  \n",
       "..                                                 ...       ...  \n",
       "925  Since the inception of [/r/pickup](/r/pickup) ...    pickup  \n",
       "926  Approaches 12-24 were all sorts of weird awkwa...    pickup  \n",
       "927  *This was written a while ago.. on DLHQ, befor...    pickup  \n",
       "928  Here is a recording of my sarge. I'm pretty ne...    pickup  \n",
       "929  This is purely for my own benefit, so that I m...    pickup  \n",
       "930                                                       pickup  \n",
       "931                                                       pickup  \n",
       "932  **TL:DR at the end**\\n\\n**Preface**\\n\\nSo earl...    pickup  \n",
       "933  **Preamble:**\\n\\nfriday night was a bust.  i h...    pickup  \n",
       "934                                                       pickup  \n",
       "935                                                       pickup  \n",
       "936                                                       pickup  \n",
       "937                                                       pickup  \n",
       "938  I'm attending an introductory course on NLP by...    pickup  \n",
       "939  This is a 3rd part of this FR series. See prev...    pickup  \n",
       "940  I've compiled a set of 335 flash cards to broa...    pickup  \n",
       "941  PRELUDE:\\n\\nI finished my school for the day. ...    pickup  \n",
       "942  Ran into HBEngineer last week. I had met her t...    pickup  \n",
       "943  So I decided to take a moment to point out a c...    pickup  \n",
       "944  Hey boys, got some quick questions and deep sh...    pickup  \n",
       "945  Sup playas, just wanted to chime in and bump a...    pickup  \n",
       "946                                                       pickup  \n",
       "947  I really feel like I'm starting to flood the s...    pickup  \n",
       "948  **Where do you daygame if you're not in colleg...    pickup  \n",
       "949  Yuck... I have spent the past two days wanderi...    pickup  \n",
       "950                                                       pickup  \n",
       "951  Excited to be posting my first FR, gents, but ...    pickup  \n",
       "952  **I'm OK, You're OK** by Thomas Harris is prob...    pickup  \n",
       "953  Hey /r/pickup, My name is Hypnotonic and here ...    pickup  \n",
       "954  Yo. So there's this girl that I met at the beg...    pickup  \n",
       "\n",
       "[955 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUAs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup\n",
      "Get request successful.\n",
      "Initiating Scrape...\n",
      "Current After:t3_a6pun3 Round: 1\n",
      "Current After:t3_a0q8qa Round: 2\n",
      "Current After:t3_9uycs6 Round: 3\n",
      "Current After:t3_9odo5z Round: 4\n",
      "Current After:t3_999npr Round: 5\n",
      "Current After:t3_8xxu7h Round: 6\n",
      "Current After:t3_8kx0rc Round: 7\n",
      "Current After:t3_8du9ob Round: 8\n",
      "Current After:t3_8a423n Round: 9\n",
      "Current After:t3_83f03b Round: 10\n",
      "Current After:t3_7wxnld Round: 11\n",
      "Current After:t3_7ssl8v Round: 12\n",
      "Current After:t3_7ovrfk Round: 13\n",
      "Current After:t3_7mdph2 Round: 14\n",
      "Current After:t3_7l1fvr Round: 15\n",
      "Current After:t3_7ixilm Round: 16\n",
      "Current After:t3_7g9bze Round: 17\n",
      "Current After:t3_7cys4y Round: 18\n",
      "Current After:t3_7aicbl Round: 19\n",
      "Current After:t3_77gshl Round: 20\n",
      "Current After:t3_733iyx Round: 21\n",
      "Current After:t3_6x3ft5 Round: 22\n",
      "Current After:t3_6swfcj Round: 23\n",
      "Current After:t3_6q1548 Round: 24\n",
      "Current After:t3_6m8xqt Round: 25\n",
      "Current After:t3_6k33d6 Round: 26\n",
      "Current After:t3_6jb2ym Round: 27\n",
      "Current After:t3_6hevyb Round: 28\n",
      "Current After:t3_6btgiz Round: 29\n",
      "Current After:t3_68e1fy Round: 30\n",
      "Current After:t3_65sky1 Round: 31\n",
      "Current After:t3_637kb9 Round: 32\n",
      "Current After:t3_60eqlg Round: 33\n",
      "Current After:t3_5vzcoc Round: 34\n",
      "Current After:t3_5owoez Round: 35\n",
      "Current After:t3_u934p Round: 36\n",
      "Current After:t3_tc9cf Round: 37\n",
      "Current After:t3_szaxy Round: 38\n",
      "Limit likely hit.  Returning available posts.\n"
     ]
    }
   ],
   "source": [
    "PUAs = scrape_reddit('pickup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the scrappings that we'll be actually using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get request successful.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-485789d82fd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_lesbians\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_reddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'actuallesbians'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_incels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_reddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'braincels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-01363a059796>\u001b[0m in \u001b[0;36mscrape_reddit\u001b[0;34m(the_subreddit, pages)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquick_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Get request successful.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initiating Scrape...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_lesbians = scrape_reddit('actuallesbians')\n",
    "df_incels = scrape_reddit('braincels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Subreddits to check out if there is the opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_femcels = scrape_reddit('Trufemcels')\n",
    "#df_gaybros = scrape_reddit('gaybros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Saved and available to be loaded from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "# Export to csv (Commented out to avoid re-saving errors)\n",
    "#df_lesbians.to_csv('actuallesbians_9_9_400', index=False)\n",
    "#df_incels.to_csv('braincels_9_9_400', index=False)\n",
    "#df_femcels.to_csv('trufemcels_9_9_1000', index=False)\n",
    "#df_gaybros.to_csv('gaybros_9_10_540', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from CSV\n",
    "df_lesbians = pd.read_csv('./actuallesbians_9_9_400')\n",
    "df_incels = pd.read_csv('./braincels_9_9_400')\n",
    "#df_femcels = pd.read_csv('./trufemcels_9_9_1000')\n",
    "#df_gaybros = pd.read_csv('./gaybros_9_10_540')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploritory Data Analysis\n",
    "    What are the most used words for each subreddit?\n",
    "    Are the most used words jargon?\n",
    "    How much text on average do the subredditors post?\n",
    "    How many of the posts are pictures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost No EDA has be done at this time in order to expidite the process of getting this project finished.  That which was mentioned during the presentation was from memory prior to the loss of my previous work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Using CountVectorizer &/or TF-IDF to generate features from the post text and title of posts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lesbians['selftext'].apply(text_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiations of the tokenizer, lemmatizer and Count Vectorizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = tokenizer.tokenize,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = 'english') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining and altering the dataframes to be modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the y Values\n",
    "df_lesbians['is_lesbians'] = 1\n",
    "df_incels['is_lesbians'] = 0\n",
    "\n",
    "# Concatination\n",
    "les_or_inc = pd.concat([df_lesbians.drop('subreddit',axis=1),df_incels.drop('subreddit', axis=1)])\n",
    "\n",
    "# Filling Nulls\n",
    "les_or_inc.fillna('', inplace=True)\n",
    "\n",
    "# Combining the title and selftext columns for easier Count Vectorization\n",
    "les_or_inc['all_text'] = les_or_inc['title'] + ' ' + les_or_inc['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "les_or_inc.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the X,y, as well as the tests and trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "X = les_or_inc['all_text']\n",
    "y = les_or_inc['is_lesbians']\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    random_state=76)\n",
    "# Count Vectorizing the train and test X's while fitting the Training X\n",
    "X_train = pd.DataFrame(cvec.fit_transform(X_train).todense(), columns=cvec.get_feature_names())\n",
    "X_test = pd.DataFrame(cvec.transform(X_test).todense(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "source": [
    "The baseline accuracy for this model is about 50% because one could simply guess 1 or 0 for all of the rows and get 50% correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and testing MultinomialNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9513513513513514\n",
      "Test: 0.8704453441295547\n"
     ]
    }
   ],
   "source": [
    "multi_model = MultinomialNB().fit(X_train,y_train)\n",
    "\n",
    "print(\"Train:\", multi_model.score(X_train,y_train))\n",
    "\n",
    "print(\"Test:\", multi_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and testing RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9932432432432432\n",
      "Test: 0.8218623481781376\n"
     ]
    }
   ],
   "source": [
    "rando_forest = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(\"Train:\", rando_forest.score(X_train,y_train))\n",
    "\n",
    "print(\"Test:\", rando_forest.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scrapped Code That I didn't want to delete, just incase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "source": [
    "cvec_lesbians['is_lesbians'] = 1\n",
    "cvec_incels['is_lesbians'] = 0\n",
    "\n",
    "les_or_inc = cvec_lesbians.add(cvec_incels, fill_value=0, axis=0)\n",
    "\n",
    "les_or_inc = les_or_inc.fillna(0).astype('int64')\n",
    "\n",
    "X = les_or_inc.drop(columns=['is_lesbians'])\n",
    "y = les_or_inc['is_lesbians']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    random_state=76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_lesbians' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e43e3c16ec04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_lesbians\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmysettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mXtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_lesbians' is not defined"
     ]
    }
   ],
   "source": [
    "df = df_lesbians['selftext'].map(text_prep)\n",
    "df = df.apply(text_prep)\n",
    "\n",
    "cvec = CountVectorizer(**mysettings).fit(Xtrain)\n",
    "Xtrain = cvec.transform(Xtrain)\n",
    "Xtest = cvec.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def text_prep(the_text):\n",
    "    the_text = the_text.lower()\n",
    "    the_text = tokenizer.tokenize(the_text)\n",
    "    for i in the_text:\n",
    "        new_text += re.sub(\"[^a-zA-A]\",\" \", i)\n",
    "    new_text = lemmatizer.lemmatize(new_text)\n",
    "    #the_text = [x.split() for x in the_text if not x in stopwords.words('english')]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A scrapped Function\n",
    "def cvec_it(df):\n",
    "    cvec_selftext = cvec.fit_transform(df['selftext'].fillna(' '))\n",
    "    text_df = pd.DataFrame(cvec_selftext.todense(),columns=cvec.vocabulary_)\n",
    "    cvec_title = cvec.fit_transform(df['title'].fillna(' '))\n",
    "    title_df = pd.DataFrame(cvec_title.todense(),columns=cvec.vocabulary_)\n",
    "    print('concatinating...')\n",
    "    return text_df.add(title_df, fill_value=0, axis=0).astype('int64')\n",
    "cvec_lesbians = cvec_it(df_lesbians)\n",
    "#cvec_femcels = cvec_it(df_femcels)\n",
    "cvec_incels = cvec_it(df_incels)\n",
    "#cvec_gaybros = cvec_it(df_gaybros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the X and y via accedental cvecing before the train/test split\n",
    "# X = cvec.fit_transform(les_or_inc['all_text'])\n",
    "# y = les_or_inc['is_lesbians']\n",
    "\n",
    "#Train Test Split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "#                                                    random_state=76)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train = cvec.transform(X_train)\n",
    "X_test = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_seduction = scrape_reddit('seduction')\n",
    "df_puascience = scrape_reddit('puascience')\n",
    "df_pickup = scrape_reddit('pickup')\n",
    "df_pua = scrape_reddit('pua')\n",
    "df_lgbt = scrape_reddit('lgbt')\n",
    "df_ainbow = scrape_reddit('ainbow')\n",
    "df_AskLGBT = scrape_reddit('AskLGBT')\n",
    "df_happentobegay = scrape_reddit('happentobegay')\n",
    "df_glbt = scrape_reddit('glbt')\n",
    "df_gay = scrape_reddit('gay')\n",
    "df_lgb = scrape_reddit('lgb')\n",
    "df_gayrights = scrape_reddit('gayrights')\n",
    "df_LGBTVent = scrape_reddit('LGBTVent')\n",
    "df_lgbtsex = scrape_reddit('lgbtsex')\n",
    "df_queer = scrape_reddit('queer')\n",
    "df_asexual = scrape_reddit('asexual')\n",
    "df_asexuality = scrape_reddit('asexuality')\n",
    "df_bisexual = scrape_reddit('bisexual')\n",
    "df_pansexual = scrape_reddit('pansexual')\n",
    "df_gaygeek = scrape_reddit('gaygeek')\n",
    "df_gaymers = scrape_reddit('gaymere')\n",
    "df_q4q = scrape_reddit('q4q')\n",
    "df_GaymersGoneMild = scrape_reddit('GaymersGoneMild')\n",
    "df_lgbtcirclejerk = scrape_reddit('lgbtcirclejerk')\n",
    "df_QPOC = scrape_reddit('QPOC')\n",
    "df_TransHack = scrape_reddit('TransHack')\n",
    "df_QueerFashionAdvice = scrape_reddit('QueerFashionAdvice')\n",
    "df_QueerTransmen = scrape_reddit('QueerTransmen')\n",
    "df_asktransgender = scrape_reddit('asktransgender')\n",
    "df_androgynoushotties = scrape_reddit('androgynoushotties')\n",
    "df_GirlGamers = scrape_reddit('GirlGamers')\n",
    "df_men = scrape_reddit('men')\n",
    "df_OneY = scrape_reddit('OneY')\n",
    "df_TwoXChromosomes = scrape_reddit('TwoXChromosomes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_seduction.to_csv('seduction_10_30', index = False)\n",
    "#df_puascience.to_csv('puascience_10_30', index = False)\n",
    "df_pickup.to_csv('pickup_10_30', index = False)\n",
    "#df_pua.to_csv('pua_10_30', index = False)\n",
    "df_lgbt.to_csv('lgbt_10_30', index = False)\n",
    "df_ainbow.to_csv('ainbow_10_30', index = False)\n",
    "df_AskLGBT.to_csv('AskLGBT_10_30', index = False)\n",
    "df_happentobegay.to_csv('happentobegay_10_30', index = False)\n",
    "df_glbt.to_csv('glbt_10_30', index = False)\n",
    "df_gay.to_csv('gay_10_30', index = False)\n",
    "#df_lgb.to_csv('lgb_10_30', index = False)\n",
    "df_gayrights.to_csv('gayrights_10_30', index = False)\n",
    "#df_LGBTVent.to_csv('LGBTVent_10_30', index = False)\n",
    "df_lgbtsex.to_csv('lgbtsex_10_30', index = False)\n",
    "df_queer.to_csv('queer_10_30', index = False)\n",
    "df_asexual.to_csv('asexual_10_30', index = False)\n",
    "df_asexuality.to_csv('asexuality_10_30', index = False)\n",
    "df_bisexual.to_csv('bisexual_10_30', index = False)\n",
    "df_pansexual.to_csv('pansexual_10_30', index = False)\n",
    "df_gaygeek.to_csv('gaygeek_10_30', index = False)\n",
    "#df_gaymers.to_csv('gaymere_10_30', index = False)\n",
    "df_q4q.to_csv('q4q_10_30', index = False)\n",
    "df_GaymersGoneMild.to_csv('GaymersGoneMild_10_30', index = False)\n",
    "df_lgbtcirclejerk.to_csv('lgbtcirclejerk_10_30', index = False)\n",
    "#df_QPOC.to_csv('QPOC_10_30', index = False)\n",
    "#df_TransHack.to_csv('TransHack_10_30', index = False)\n",
    "#df_QueerFashionAdvice.to_csv('QueerFashionAdvice_10_30', index = False)\n",
    "df_QueerTransmen.to_csv('QueerTransmen_10_30', index = False)\n",
    "df_asktransgender.to_csv('asktransgender_10_30', index = False)\n",
    "df_androgynoushotties.to_csv('androgynoushotties_10_30', index = False)\n",
    "df_GirlGamers.to_csv('GirlGamers_10_30', index = False)\n",
    "df_men.to_csv('men_10_30', index = False)\n",
    "df_OneY.to_csv('OneY_10_30', index = False)\n",
    "#df_TwoXChromosome.to_csv('TwoXChromosomes_10_30', index = False)\n",
    "#df_lesbians.to_csv('actuallesbians_9_9_400', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifting over to the analysis of multiple Reddits based on gender and sexuality.\n",
    "\n",
    "Female, Straight:\n",
    "Manosphere:\n",
    " - seduction\n",
    " - pickup\n",
    " - men\n",
    " - OneY\n",
    "\n",
    "Queer:\n",
    " - FEMM\n",
    " - FemmeThoughts\n",
    " - FIREyFemmes\n",
    " - MaleFemme\n",
    " - lgbt\n",
    " - ainbow\n",
    " - AskLGBT\n",
    " - happentobegay\n",
    " - glbt\n",
    " - gay\n",
    " - gayrights\n",
    " - lgbtsex\n",
    " - queer\n",
    " - asexual\n",
    " - asexuality\n",
    " - bisexual\n",
    " - pansexual\n",
    " - q4q\n",
    " - actuallesbians\n",
    " - lgbtcirclejerk\n",
    " - QueerTransmen\n",
    " - asktransgender\n",
    " - androgynoushotties\n",
    "\n",
    "Queer Geeks/Nerds:\n",
    " - gaygeek\n",
    " - GaymersGoneMild\n",
    " - GirlGamers\n",
    "\n",
    "### To be obtained\n",
    "\n",
    "Feminism:\n",
    " - AskFeminists\n",
    " - antifeminists\n",
    " - RadicalFeminism\n",
    " - DebateFeminism\n",
    " - GenderCritical\n",
    " - Feminism\n",
    "Femmes\n",
    " - FEMM\n",
    " - Femme\n",
    " - FemmeThoughts\n",
    " - FIREyFemmes\n",
    " - MaleFemme\n",
    "Butch\n",
    " - LesbianActually\n",
    " - butchlesbians\n",
    " \n",
    "Mens\n",
    " - AskMen\n",
    " - Manosphere\n",
    "QPOC\n",
    " - QueerWomenOfColor\n",
    " - gaypoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class subreddit:\n",
    "def scrape_reddit(the_subreddit, pages = 40):\n",
    "    print(str(the_subreddit))\n",
    "    all_posts = []\n",
    "    first_url = 'http://www.reddit.com/r/' + the_subreddit + '.json'\n",
    "    url = first_url\n",
    "    list_of_df = []\n",
    "    \n",
    "    #Putting in a get check, for happy sanity reasons:\n",
    "    quick_check = requests.get(first_url, headers = {'User-agent':'Electronic Goddess'})\n",
    "    if int(str(quick_check)[11:14]) == 200:\n",
    "        print(\"Get request successful.\")\n",
    "        time.sleep(3)\n",
    "        print(\"Initiating Scrape...\")\n",
    "    else:\n",
    "        print(\"Get request not 200, instead recieved:\" + str(quick_check))\n",
    "        return\n",
    "    \n",
    "    #Now for the actual Scraping:\n",
    "    for round in range(pages):\n",
    "        try:\n",
    "            res = requests.get(url, headers = {'User-agent':'Electronic Goddess'})\n",
    "            data = res.json()\n",
    "            list_of_posts = data['data']['children']\n",
    "            all_posts = all_posts + list_of_posts\n",
    "            after = data['data']['after']\n",
    "            url = first_url +'?after=' + after\n",
    "            print('Current After:' + after,'Round: '+ str(round + 1))\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            print('Limit likely hit.  Returning available posts.')\n",
    "            break\n",
    "#        return all_posts # This can be un-commented out incase I want the straight forward raw scrape\n",
    "\n",
    "    #Formats the parts we care about into a list of dictionaries that'll become the dataframe\n",
    "    for i in range(len(all_posts)):\n",
    "        index_dictionary = {\n",
    "                'title' : all_posts[i]['data']['title'],\n",
    "                'selftext': all_posts[i]['data']['selftext'],\n",
    "                'subreddit' : all_posts[i]['data']['subreddit']\n",
    "            }\n",
    "        list_of_df.append(index_dictionary)\n",
    "    return pd.DataFrame(list_of_df, columns = ['title','selftext','subreddit'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_spelled_out(df_column, num_topics = 3, num_words = 5):\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics,\n",
    "                                    learning_method='online') # Learning meathod stated for depreciation\n",
    "    #cvec.fit(df_column)\n",
    "    text = cvec.fit_transform(df_column)\n",
    "    lda.fit(text)\n",
    "    for ix, topic in enumerate(lda.components_):\n",
    "        display(pd.DataFrame())\n",
    "        print('Topic ', ix + 1)\n",
    "        top_words = [cvec.get_feature_names()[i] for i in lda.components_[ix].argsort()[:-num_words - 1:-1]]\n",
    "        print('\\n'.join(top_words))\n",
    "        print('\\n')\n",
    "    return`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
